{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPvg5D2mzz7ZB+X3PZw08/0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Skin lesion classification of dermoscopic images using machine learning and convolutional neural network\n","\n","19 December 2022\n","\n","https://www.nature.com/articles/s41598-022-22644-9#Tab7\n","\n","https://aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=realm&dataSetSn=561"],"metadata":{"id":"SoOMF4kHSwMS"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EzLpCjbDOZy_","executionInfo":{"status":"ok","timestamp":1690959297007,"user_tz":-540,"elapsed":6522,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"0e6af8be-8175-4db9-ecee-b8fc7ce7f1ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{"id":"sALkX2o9Syb_"}},{"cell_type":"code","source":["import glob\n","import os\n","import json\n","import numpy as np\n","import cv2\n","import pandas as pd\n","\n","# 데이터 로딩 및 정렬: 디렉토리에서 이미지와 JSON 경로를 찾아 정렬된 순으로 반환\n","def preprocess_data(directory):\n","    image_paths = glob.glob(os.path.join(directory, '**', '*.jpg'), recursive=True)\n","    json_paths = glob.glob(os.path.join(directory, '**', '*.json'), recursive=True)\n","\n","    image_paths.sort()\n","    json_paths.sort()\n","\n","    return image_paths, json_paths\n","\n","# 이미지 처리: 이미지 및 JSON 정보를 사용하여 크롭 및 리사이징 작업 수행하고 데이터프레임으로 반환\n","def process_images(image_paths, json_paths):\n","    cropped_images = []\n","    metadata_list = []\n","\n","    for image_path, json_path in zip(image_paths, json_paths):\n","        with open(json_path) as f:\n","            json_data = json.load(f)\n","\n","        image = cv2.imread(image_path)\n","\n","        if json_data.get('metaData'):\n","            metadata_list.append(json_data['metaData'])\n","\n","        if json_data.get('labelingInfo'):\n","            for label_info in json_data['labelingInfo']:\n","                if label_info.get('box'):\n","                    coordinates = label_info['box']\n","                    cropped_image = crop_image(image, coordinates)\n","                    resized_cropped_image = resize_image(cropped_image)\n","                    cropped_images.append({\"cropped_box_image\": resized_cropped_image})\n","                if label_info.get('polygon'):\n","                    coordinates = label_info['polygon']\n","                    cropped_image = crop_image(image, coordinates)\n","                    resized_cropped_image = resize_image(cropped_image)\n","                    cropped_images.append({\"cropped_polygon_image\": resized_cropped_image})\n","\n","    metadata_df = pd.DataFrame(metadata_list)\n","    cropped_images_df = pd.concat([pd.DataFrame(cropped_images[i]) for i in range(len(cropped_images))], keys=cropped_images, axis=1)\n","    images_metadata_df = pd.concat([metadata_df, cropped_images_df], axis=1)\n","\n","    return images_metadata_df\n","\n","# 결과 저장 및 출력: 각각에 대해 데이터프레임을 생성하고 로컬 경로에 CSV 파일로 저장\n","def save_results(result_df):\n","    metadata_columns = len(result_df.columns) - 2\n","    result_file_names = [\n","        \"polygon_cropped.csv\",\n","        \"box_cropped.csv\",\n","        \"polygon_box_cropped.csv\",\n","        \"polygon_cropped_metadata.csv\",\n","        \"box_cropped_metadata.csv\",\n","        \"polygon_box_cropped_metadata.csv\",\n","    ]\n","\n","    result_dataframes = [\n","        result_df.iloc[:, [-2]],  # 폴리곤 크롭 이미지\n","        result_df.iloc[:, [-1]],  # 박스 크롭 이미지\n","        result_df.iloc[:, [-2, -1]],  # 폴리곤 크롭 이미지 및 박스 크롭 이미지\n","        result_df.iloc[:, list(range(metadata_columns)) + [-2]],  # 폴리곤 크롭 이미지와 메타데이터\n","        result_df.iloc[:, list(range(metadata_columns)) + [-1]],  # 박스 크롭 이미지와 메타데이터\n","        result_df.iloc[:, list(range(metadata_columns)) + [-2, -1]],  # 폴리곤 크롭 이미지, 박스 크롭 이미지와 메타데이터\n","    ]\n","\n","    for file_name, dataframe in zip(result_file_names, result_dataframes):\n","        dataframe.to_csv(file_name, index=False)\n","\n","src_path = '/content/drive/Shareddrives/152.반려동물 피부질환 데이터'\n","\n","# 파이프라인 실행\n","image_paths, json_paths = preprocess_data(src_path)\n","result_df = process_images(image_paths, json_paths)\n","save_results(result_df)"],"metadata":{"id":"xn9z49K-CnXK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def resize_image(image, target_size=(128, 128)):\n","    return cv2.resize(image, target_size)\n","\n","def create_binary_mask(image, coordinates):\n","    mask = np.zeros_like(image)\n","    if len(coordinates) > 2: # 폴리곤 경우\n","        points = np.array([coordinates], np.int32)\n","        mask = cv2.fillPoly(mask, points, (255, 255, 255))\n","    else: # 박스 경우\n","        x, y, w, h = coordinates\n","        mask[y:y+h, x:x+w] = (255, 255, 255)\n","    return cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n","\n","def create_segmentation_map(image, coordinates):\n","    segmentation_map = np.zeros_like(image)\n","    for idx, coord in enumerate(coordinates):\n","        if len(coord) > 2: # 폴리곤 경우\n","            points = np.array([coord], np.int32)\n","            segmentation_map = cv2.fillPoly(segmentation_map, points, (idx+1, idx+1, idx+1))\n","        else: # 박스 경우\n","            x, y, w, h = coord\n","            segmentation_map[y:y+h, x:x+w] = (idx+1, idx+1, idx+1)\n","    return cv2.cvtColor(segmentation_map, cv2.COLOR_BGR2GRAY)\n","\n","def create_masks_and_maps(image, coordinates, resize=False):\n","    if resize:\n","        resized_image = resize_image(image)\n","        binary_mask = create_binary_mask(resized_image, coordinates)\n","        segmentation_map = create_segmentation_map(resized_image, coordinates)\n","    else:\n","        binary_mask = create_binary_mask(image, coordinates)\n","        segmentation_map = create_segmentation_map(image, coordinates)\n","    return binary_mask, segmentation_map\n","\n","# 데이터 로딩 및 정렬: 디렉토리에서 이미지와 JSON 경로를 찾아 정렬된 순으로 반환\n","def preprocess_data(directory):\n","    image_paths = glob.glob(os.path.join(directory, '**', '*.jpg'), recursive=True)\n","    json_paths = glob.glob(os.path.join(directory, '**', '*.json'), recursive=True)\n","\n","    image_paths.sort()\n","    json_paths.sort()\n","\n","    return image_paths, json_paths\n","\n","# 이미지 처리: 이미지 및 JSON 정보를 사용하여 이진 마스크 및 segmentation map을 생성하고 데이터프레임으로 반환\n","def process_images(image_paths, json_paths):\n","    metadata_list = []\n","\n","    results = {\n","        \"before_resize\": [],\n","        \"after_resize\": [],\n","    }\n","\n","    for image_path, json_path in zip(image_paths, json_paths):\n","        with open(json_path) as f:\n","            json_data = json.load(f)\n","\n","        image = cv2.imread(image_path)\n","\n","        if json_data.get('metaData'):\n","            metadata_list.append(json_data['metaData'])\n","\n","        # 폴리곤 및 박스 좌표 처리 및 이진 마스크/segmentation map 생성, 리사이징 처리\n","        if json_data.get('labelingInfo'):\n","            before_resize = []\n","            after_resize = []\n","\n","            for label_info in json_data['labelingInfo']:\n","                if label_info.get('box') or label_info.get('polygon'):\n","                    coordinates = label_info['box'] if label_info.get('box') else label_info['polygon']\n","\n","                    # create_binary_mask, create_segmentation_map은 각각 이진 마스크, segmentation map 생성을 위한 함수입니다.\n","                    binary_mask_before_resize, segmentation_map_before_resize = create_masks_and_maps(image, coordinates)\n","                    resized_image = resize_image(image)\n","                    binary_mask_after_resize, segmentation_map_after_resize = create_masks_and_maps(resized_image, coordinates)\n","\n","\n","                    before_resize.append({\n","                        \"before_resize_binary_mask\": binary_mask_before_resize,\n","                        \"before_resize_segmentation_map\": segmentation_map_before_resize,\n","                    })\n","\n","                    after_resize.append({\n","                        \"after_resize_binary_mask\": binary_mask_after_resize,\n","                        \"after_resize_segmentation_map\": segmentation_map_after_resize,\n","                    })\n","\n","            results[\"before_resize\"].append(before_resize)\n","            results[\"after_resize\"].append(after_resize)\n","\n","    metadata_df = pd.DataFrame(metadata_list)\n","\n","    # 결과 데이터프레임 및 저장 파일 이름 정의\n","    result_dataframes = []\n","    result_file_names = []\n","\n","    for key, values in results.items():\n","        for i, value in enumerate(values):\n","            for j, (_, content) in enumerate(value.items()):\n","                result_dataframes.append(pd.DataFrame({f\"{key}_{i}_{j}\": content}))\n","                result_file_names.append(f\"{key}_{i}_{j}.csv\")\n","\n","    # 메타데이터와 결합하여 최종 결과 데이터프레임 생성 및 저장\n","    for file_name, dataframe, _ in zip(result_file_names, result_dataframes, results.values()):\n","        result_df = pd.concat([metadata_df, dataframe], axis=1)\n","        result_df.to_csv(file_name, index=False)\n","\n","src_path = '/content/drive/Shareddrives/152.반려동물 피부질환 데이터'\n","\n","image_paths, json_paths = preprocess_data(src_path)\n","process_images(image_paths, json_paths)"],"metadata":{"id":"wh9iSOZ6QnEl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import cv2\n","import glob\n","import numpy as np\n","import pandas as pd\n","\n","class ImageProcessor:\n","    def __init__(self, src_path):\n","        self.src_path = src_path\n","        self.image_paths, self.json_paths = self.preprocess_data()\n","\n","    def resize_image(self, image, target_size=(96, 96)):\n","        return cv2.resize(image, target_size)\n","\n","    def create_binary_mask(self, image, coordinates):\n","        mask = np.zeros_like(image)\n","        if len(coordinates) > 2: # 폴리곤 경우\n","            points = np.array([coordinates], np.int32)\n","            mask = cv2.fillPoly(mask, points, (255, 255, 255))\n","        else: # 박스 경우\n","            x, y, w, h = coordinates\n","            mask[y:y+h, x:x+w] = (255, 255 ,255)\n","        return cv2.cvtColor(mask,cv2.COLOR_BGR2GRAY)\n","\n","    def create_segmentation_map(self,image ,coordinates):\n","        segmentation_map=np.zeros_like(image)\n","\n","        for idx ,coord in enumerate(coordinates):\n","            if len(coord)>2: # 폴리곤 경우\n","                points=np.array([coord],np.int32)\n","                segmentation_map=cv2.fillPoly(segmentation_map ,points ,(idx+1,idx+1,idx+1))\n","\n","            else: # 박스 경우\n","                x,y,w,h=coord\n","                segmentation_map[y:y+h,x:x+w]=(idx+1,idx+1,idx+1)\n","\n","        return cv2.cvtColor(segmentation_map,cv2.COLOR_BGR2GRAY)\n","\n","    def create_masks_and_maps(self,image ,coordinates ,resize=False):\n","\n","      if resize:\n","          resized_image=self.resize_image(image)\n","          binary_mask=self.create_binary_mask(resized_image ,coordinates)\n","          segmentation_map=self.create_segmentation_map(resized_image ,coordinates)\n","\n","      else:\n","          binary_mask=self.create_binary_mask(image ,coordinates)\n","          segmentation_map=self.create_segmentation_map(image ,coordinates)\n","\n","      return binary_mask,segmentation_map\n","\n","    def preprocess_data(self):\n","        image_paths=glob.glob(os.path.join(self.src_path ,'**','*.jpg') ,recursive=True)\n","        json_paths=glob.glob(os.path.join(self.src_path ,'**','*.json') ,recursive=True)\n","\n","        image_paths.sort()\n","        json_paths.sort()\n","\n","        return image_paths,json_paths\n","\n","    def process_images(self):\n","\n","      metadata_list=[]\n","\n","      results={\n","          \"before_resize\":[],\n","          \"after_resize\":[]\n","      }\n","\n","      for image_path,json_path in zip(self.image_paths,self.json_paths):\n","\n","          with open(json_path) as f:\n","              json_data=json.load(f)\n","\n","          image=cv2.imread(image_path)\n","\n","          if json_data.get('metaData'):\n","              metadata_list.append(json_data['metaData'])\n","\n","          if json_data.get('labelingInfo'):\n","              before_resize=[]\n","              after_resize=[]\n","\n","              for label_info in json_data['labelingInfo']:\n","\n","                  if label_info.get('box') or label_info.get('polygon'):\n","                      coordinates=label_info['box'] if label_info.get('box') else label_info['polygon']\n","\n","                      binary_mask_before_resize,segmentation_map_before_resize=self.create_masks_and_maps(image,coordinates)\n","\n","                      resized_image=self.resize_image(image)\n","                      binary_mask_after_resize,segmentation_map_after_resize=self.create_masks_and_maps(resized_image,coordinates)\n","\n","\n","                      before_resize.append({\n","                          \"before_resize_binary_mask\":binary_mask_before_resize,\n","                          \"before_resize_segmentation_map\":segmentation_map_before_resize,\n","                      })\n","\n","                      after_resize.append({\n","                          \"after_resize_binary_mask\":binary_mask_after_resize,\n","                          \"after_resize_segmentation_map\":segmentation_map_after_resize,\n","                      })\n","\n","              results[\"before_resize\"].append(before_resize)\n","              results[\"after_resize\"].append(after_resize)\n","\n","      metadata_df=pd.DataFrame(metadata_list)\n","\n","      result_dataframes=[]\n","      result_file_names=[]\n","\n","      for key,values in results.items():\n","          for i,value in enumerate(values):\n","              for j,(_,content) in enumerate(value.items()):\n","                  result_dataframes.append(pd.DataFrame({f\"{key}_{i}_{j}\":content}))\n","                  result_file_names.append(f\"{key}_{i}_{j}.csv\")\n","\n","      for file_name,dataframe,_ in zip(result_file_names,result_dataframes,results.values()):\n","          result_df=pd.concat([metadata_df,dataframe],axis=1)\n","          result_df.to_csv(file_name,index=False)\n","\n","processor=ImageProcessor('/content/drive/Shareddrives/152.반려동물 피부질환 데이터')\n","processor.process_images()"],"metadata":{"id":"61EKKHyXX1Rg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Modeling"],"metadata":{"id":"auDavqpSS5rc"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import models\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","import pandas as pd\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"9KN24A9E1JFc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. 폴리곤 크롭 이미지\n","polygon_cropped = pd.read_csv(\"polygon_cropped.csv\")\n","\n","# 2. 박스 크롭 이미지\n","box_cropped = pd.read_csv(\"box_cropped.csv\")\n","\n","# 3. 폴리곤 크롭 이미지, 박스 크롭 이미지\n","polygon_box_cropped = pd.read_csv(\"polygon_box_cropped.csv\")\n","\n","# 4. 폴리곤 크롭 이미지, 메타데이터\n","polygon_cropped_metadata = pd.read_csv(\"polygon_cropped_metadata.csv\")\n","\n","# 5. 박스 크롭 이미지, 메타데이터\n","box_cropped_metadata = pd.read_csv(\"box_cropped_metadata.csv\")\n","\n","# 6. 폴리곤 크롭 이미지, 박스 크롭 이미지, 메타데이터\n","polygon_box_cropped_metadata = pd.read_csv(\"polygon_box_cropped_metadata.csv\")"],"metadata":{"id":"0PNTCTkPz0gx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"s4QcwK9UBb2u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 분류 모델과 하이퍼파라미터를 설정합니다\n","model_LR = LogisticRegression(random_state=9)\n","model_LDA = LinearDiscriminantAnalysis(solver='svd')\n","model_KNN = KNeighborsClassifier(n_neighbors=5)\n","model_DT = DecisionTreeClassifier(n_estimators=100)\n","model_RF = RandomForestClassifier(n_estimators=200, random_state=0)\n","model_GaussianNB = GaussianNB(var_smoothing=1e-09)\n","model_SVM = SVC(kernel='linear', C=1, random_state=0)\n","\n","# 분류 모델들을 리스트에 담습니다\n","models = [model_LR, model_LDA, model_KNN, model_DT, model_RF, model_GaussianNB, model_SVM]\n","\n","# 각 분류 모델을 학습시키고 예측 결과를 출력합니다\n","for model in models:\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    score = accuracy_score(y_test, y_pred)\n","    print(f\"{model.__class__.__name__}: {score}\")"],"metadata":{"id":"eK7BGzioShjF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KutoBNj2_Iuw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sequential 모델 생성\n","model = models.Sequential()\n","\n","# 첫번째 Conv2D 레이어\n","model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(96,96,3)))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(3, 3)))\n","\n","# 첫번째 Dropout 레이어\n","model.add(Dropout(0.25))\n","\n","# 두번째 Conv2D 레이어\n","model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# 두번째 Dropout 레이어\n","model.add(Dropout(0.25))\n","\n","# 세번째 Conv2D 레이어\n","model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# 세번째 Dropout 레이어\n","model.add(Dropout(0.25))\n","\n","# Flatten 레이어\n","model.add(Flatten())\n","\n","# 첫번째 Dense 레이어\n","model.add(Dense(units=1024, activation='relu'))\n","model.add(BatchNormalization())\n","\n","# 두번째 Dropout 레이어\n","model.add(Dropout(0.5))\n","\n","# 두번째 Dense 레이어: 최종 출력 레이어\n","model.add(Dense(units=7, activation='softmax'))\n","\n","# 모델 컴파일\n","opt = Adam(lr=0.001, decay=0.00001)\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","# 모델 구조 요약\n","model.summary()\n","\n","# 모델 학습\n","epochs = 150\n","batch_size = 32\n","\n","history = model.fit(train_data, epochs=epochs, batch_size=batch_size, validation_data=val_data)\n","\n","# 모델 평가\n","test_loss, test_acc = model.evaluate(test_data)\n","print(\"Test Loss:\", test_loss)\n","print(\"Test Accuracy:\", test_acc)"],"metadata":{"id":"IZOVWv80Sea6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EG-vxlAj_HYD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TensorFlow Lite 모델로 변환\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","tflite_model = converter.convert()\n","\n","# 변환된 모델을 파일로 저장\n","with open('your_model.tflite', 'wb') as f:\n","    f.write(tflite_model)"],"metadata":{"id":"f0qJBmnqSbr7"},"execution_count":null,"outputs":[]}]}