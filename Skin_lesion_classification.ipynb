{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyP01J/gTkFx/riUO7kLx1rf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Skin lesion classification of dermoscopic images using machine learning and convolutional neural network\n","\n","19 December 2022\n","\n","https://www.nature.com/articles/s41598-022-22644-9#Tab7\n","\n","https://aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=realm&dataSetSn=561"],"metadata":{"id":"SoOMF4kHSwMS"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EzLpCjbDOZy_","executionInfo":{"status":"ok","timestamp":1691469456007,"user_tz":-540,"elapsed":3499,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"39f6b611-ae73-42f7-f4d3-9eb65390d535"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{"id":"sALkX2o9Syb_"}},{"cell_type":"code","source":["import cv2\n","import json\n","import itertools\n","import glob\n","import os\n","import numpy as np\n","import pandas as pd\n","\n","class ImageProcessor:\n","    def __init__(self, src_path):\n","        self.src_path = src_path\n","        self.image_paths, self.json_paths = self.preprocess_data()\n","\n","    def resize_image(self, image, target_size=(96, 96)):\n","        return cv2.resize(image, target_size)\n","\n","    def create_masks_and_maps(self, image, coordinates, mask_type):\n","        mask = np.zeros_like(image)\n","\n","        for idx, coord in enumerate(coordinates):\n","            if len(coord) > 2: # 폴리곤 경우\n","                points = np.array([coord], np.int32)\n","                color = (255, 255, 255) if mask_type == \"binary_mask\" else (idx + 1, idx + 1, idx + 1)\n","                mask = cv2.fillPoly(mask, points, color)\n","            else: # 박스 경우\n","                x, y, w, h = coord\n","                color = (255, 255, 255) if mask_type == \"binary_mask\" else (idx + 1, idx + 1, idx + 1)\n","                mask[y:y + h, x:x + w] = color\n","\n","        return cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n","\n","    def preprocess_data(self):\n","        image_paths = glob.glob(os.path.join(self.src_path, '**', '*.jpg'), recursive=True)\n","        json_paths = glob.glob(os.path.join(self.src_path, '**', '*.json'), recursive=True)\n","        image_paths.sort()\n","        json_paths.sort()\n","        return image_paths, json_paths\n","\n","    def process_images(self):\n","        results = {}\n","\n","        # .polygon. .box.을 구체화해서 추가\n","        for image_path, json_path in zip(self.image_paths, self.json_paths):\n","            with open(json_path) as f:\n","                json_data = json.load(f)\n","\n","            image = cv2.imread(image_path)\n","            metadata_df = pd.DataFrame(json_data['metaData'], index=[0])\n","            resized_image = self.resize_image(image)\n","\n","            labeling_types = [('polygon', 'polygon'), ('box', 'box'), ('polygon_box', ['box', 'polygon'])]\n","\n","            coordinates = []\n","            for label_info in json_data['labelingInfo']:\n","                for label_key in ['polygon', 'box']:\n","                    if label_info.get(label_key):\n","                        coord = label_info[label_key]['location']\n","                        if isinstance(coord, list):\n","                            coordinates.extend(coord)\n","                        else:\n","                            coordinates.append(coord)\n","\n","                for mask_type in ['binary_mask', 'segmentation_map']:\n","                    for resize_option in ['before_resize', 'after_resize']:\n","                        img = resized_image if resize_option == 'after_resize' else image\n","                        mask = self.create_masks_and_maps(img, coordinates, mask_type)\n","\n","                        for metadata_option in ['no_metadata', 'metadata']:\n","                            key = (resize_option, label_type, metadata_option, mask_type)\n","                            if key not in results:\n","                                results[key] = []\n","\n","                            output_df = pd.DataFrame({f\"{key}\": [mask]})\n","                            result = pd.concat([metadata_df, output_df], axis=1) if metadata_option == 'metadata' else output_df\n","                            results[key].append(result)\n","\n","        result_dataframes = []\n","\n","        # 결과 파일 이름 형식 추가\n","        file_name_format = \"{resize_option}_{label_type}_{mask_type}{meta_data_suffix}.csv\"\n","\n","        for key, result_list in results.items():\n","            resize_option, label_type, metadata_option, mask_type = key\n","            meta_data_suffix = \"_MetaData\" if metadata_option == 'metadata' else \"\"\n","            file_name = file_name_format.format(resize_option=resize_option, label_type=label_type,\n","                                                mask_type=mask_type, meta_data_suffix=meta_data_suffix)\n","            combined_df = pd.concat(result_list, ignore_index=True)\n","            combined_df.to_csv(file_name, index=False)\n","            result_dataframes.append(combined_df)\n","\n","        return result_dataframes"],"metadata":{"id":"WYXEMpJVF24P","executionInfo":{"status":"ok","timestamp":1691475469182,"user_tz":-540,"elapsed":2,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["processor = ImageProcessor('/content/drive/Shareddrives/152.반려동물 피부질환 데이터/일반카메라/2.Validation/반려묘')\n","processor.process_images()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":372},"id":"kD_b1PTkcEJP","executionInfo":{"status":"error","timestamp":1691475473598,"user_tz":-540,"elapsed":1258,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"1a03f46b-dd73-42e4-d28b-a19d74ba832a"},"execution_count":9,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-924c18e8effa>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/Shareddrives/152.반려동물 피부질환 데이터/일반카메라/2.Validation/반려묘'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-8b5d8133c61c>\u001b[0m in \u001b[0;36mprocess_images\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mresize_option\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'before_resize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'after_resize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresized_image\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresize_option\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'after_resize'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_masks_and_maps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoordinates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mmetadata_option\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'no_metadata'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'metadata'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-8b5d8133c61c>\u001b[0m in \u001b[0;36mcreate_masks_and_maps\u001b[0;34m(self, image, coordinates, mask_type)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoordinates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 폴리곤 경우\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmask_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary_mask\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillPoly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'dict'"]}]},{"cell_type":"code","source":["Before_Resize_Polygon_Binary_Mask.csv\n","After_Resize_Polygon_Binary_Mask.csv\n","Before_Resize_Box_Binary_Mask.csv\n","After_Resize_Box_Binary_Mask.csv\n","Before_Resize_Polygon_Segmentation_Map.csv\n","After_Resize_Polygon_Segmentation_Map.csv\n","Before_Resize_Box_Segmentation_Map.csv\n","After_Resize_Box_Segmentation_Map.csv\n","Before_Resize_Polygon_Box_Binary_Mask.csv\n","After_Resize_Polygon_Box_Binary_Mask.csv\n","Before_Resize_Polygon_Box_Segmentation_Map.csv\n","After_Resize_Polygon_Box_Segmentation_Map.csv\n","Before_Resize_Polygon_Binary_Mask_MetaData.csv\n","After_Resize_Polygon_Binary_Mask_MetaData.csv\n","Before_Resize_Box_Binary_Mask_MetaData.csv\n","After_Resize_Box_Binary_Mask_MetaData.csv\n","Before_Resize_Polygon_Segmentation_Map_MetaData.csv\n","After_Resize_Polygon_Segmentation_Map_MetaData.csv\n","Before_Resize_Box_Segmentation_Map_MetaData.csv\n","After_Resize_Box_Segmentation_Map_MetaData.csv"],"metadata":{"id":"CsVelS76GYKV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Modeling"],"metadata":{"id":"auDavqpSS5rc"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import models\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","import pandas as pd\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"9KN24A9E1JFc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. 폴리곤 크롭 이미지\n","polygon_cropped = pd.read_csv(\"polygon_cropped.csv\")\n","\n","# 2. 박스 크롭 이미지\n","box_cropped = pd.read_csv(\"box_cropped.csv\")\n","\n","# 3. 폴리곤 크롭 이미지, 박스 크롭 이미지\n","polygon_box_cropped = pd.read_csv(\"polygon_box_cropped.csv\")\n","\n","# 4. 폴리곤 크롭 이미지, 메타데이터\n","polygon_cropped_metadata = pd.read_csv(\"polygon_cropped_metadata.csv\")\n","\n","# 5. 박스 크롭 이미지, 메타데이터\n","box_cropped_metadata = pd.read_csv(\"box_cropped_metadata.csv\")\n","\n","# 6. 폴리곤 크롭 이미지, 박스 크롭 이미지, 메타데이터\n","polygon_box_cropped_metadata = pd.read_csv(\"polygon_box_cropped_metadata.csv\")"],"metadata":{"id":"0PNTCTkPz0gx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"s4QcwK9UBb2u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 분류 모델과 하이퍼파라미터를 설정합니다\n","model_LR = LogisticRegression(random_state=9)\n","model_LDA = LinearDiscriminantAnalysis(solver='svd')\n","model_KNN = KNeighborsClassifier(n_neighbors=5)\n","model_DT = DecisionTreeClassifier(n_estimators=100)\n","model_RF = RandomForestClassifier(n_estimators=200, random_state=0)\n","model_GaussianNB = GaussianNB(var_smoothing=1e-09)\n","model_SVM = SVC(kernel='linear', C=1, random_state=0)\n","\n","# 분류 모델들을 리스트에 담습니다\n","models = [model_LR, model_LDA, model_KNN, model_DT, model_RF, model_GaussianNB, model_SVM]\n","\n","# 각 분류 모델을 학습시키고 예측 결과를 출력합니다\n","for model in models:\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    score = accuracy_score(y_test, y_pred)\n","    print(f\"{model.__class__.__name__}: {score}\")"],"metadata":{"id":"eK7BGzioShjF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KutoBNj2_Iuw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sequential 모델 생성\n","model = models.Sequential()\n","\n","# 첫번째 Conv2D 레이어\n","model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(96,96,3)))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(3, 3)))\n","\n","# 첫번째 Dropout 레이어\n","model.add(Dropout(0.25))\n","\n","# 두번째 Conv2D 레이어\n","model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# 두번째 Dropout 레이어\n","model.add(Dropout(0.25))\n","\n","# 세번째 Conv2D 레이어\n","model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# 세번째 Dropout 레이어\n","model.add(Dropout(0.25))\n","\n","# Flatten 레이어\n","model.add(Flatten())\n","\n","# 첫번째 Dense 레이어\n","model.add(Dense(units=1024, activation='relu'))\n","model.add(BatchNormalization())\n","\n","# 두번째 Dropout 레이어\n","model.add(Dropout(0.5))\n","\n","# 두번째 Dense 레이어: 최종 출력 레이어\n","model.add(Dense(units=7, activation='softmax'))\n","\n","# 모델 컴파일\n","opt = Adam(lr=0.001, decay=0.00001)\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","# 모델 구조 요약\n","model.summary()\n","\n","# 모델 학습\n","epochs = 150\n","batch_size = 32\n","\n","history = model.fit(train_data, epochs=epochs, batch_size=batch_size, validation_data=val_data)\n","\n","# 모델 평가\n","test_loss, test_acc = model.evaluate(test_data)\n","print(\"Test Loss:\", test_loss)\n","print(\"Test Accuracy:\", test_acc)"],"metadata":{"id":"IZOVWv80Sea6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EG-vxlAj_HYD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TensorFlow Lite 모델로 변환\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","tflite_model = converter.convert()\n","\n","# 변환된 모델을 파일로 저장\n","with open('your_model.tflite', 'wb') as f:\n","    f.write(tflite_model)"],"metadata":{"id":"f0qJBmnqSbr7"},"execution_count":null,"outputs":[]}]}