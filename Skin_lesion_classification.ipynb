{"cells":[{"cell_type":"markdown","metadata":{"id":"SoOMF4kHSwMS"},"source":["# Skin lesion classification of dermoscopic images using machine learning and convolutional neural network\n","\n","19 December 2022\n","\n","https://www.nature.com/articles/s41598-022-22644-9#Tab7\n","\n","https://aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=realm&dataSetSn=561"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31524,"status":"ok","timestamp":1693527437697,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"EzLpCjbDOZy_","outputId":"a1eb49e0-5930-4f76-8f23-841ac8a7cd57"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import glob\n","import json\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from sklearn.metrics import classification_report\n","from sklearn.preprocessing import LabelBinarizer, OneHotEncoder\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras import models\n","from tensorflow.keras.layers import BatchNormalization, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import EarlyStopping"],"metadata":{"id":"491U9sI3TVVj","executionInfo":{"status":"ok","timestamp":1693534052132,"user_tz":-540,"elapsed":4,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def get_image_and_json_paths(src_path, num_files):\n","    image_paths = []\n","    json_paths = []\n","\n","    for root, dirs, files in tqdm(os.walk(src_path), desc='Walking directories', unit=' dir'):\n","        for dir in dirs:\n","            dir_path = os.path.join(root, dir)\n","            image_paths.extend(sorted(glob.glob(os.path.join(dir_path, '*.jpg'))[:num_files]))\n","            json_paths.extend(sorted(glob.glob(os.path.join(dir_path, '*.json'))[:num_files]))\n","\n","    return image_paths, json_paths\n","\n","def extract_data_from_json(json_paths):\n","    lesions, polygon_locations, box_locations = [], [], []\n","    for json_path in tqdm(json_paths, desc='Loading JSON', unit=' file'):\n","        try:\n","            with open(json_path, 'r', encoding='utf-8') as file:\n","                json_data = json.loads(re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', file.read()))\n","                metadata, labeling_info = json_data.get('metaData', None), json_data['labelingInfo']\n","\n","                lesions.append(metadata.get('lesions', None))\n","\n","                for entry in labeling_info:\n","                    if 'polygon' in entry:\n","                        polygon_locations.extend(entry['polygon'].get('location', None))\n","                    if 'box' in entry:\n","                        box_locations.extend(entry['box'].get('location', None))\n","\n","        except Exception as e:\n","            print(json_path)\n","            print(e)\n","\n","    return lesions, polygon_locations, box_locations\n","\n","def preprocess_image(image, target_size):\n","    image = tf.image.resize(image, target_size)\n","    image = tf.cast(image, tf.float32) / 255.0\n","    return image\n","\n","def create_dataset(image_paths, lesions, buffer_size, batch_size, target_size):\n","    image_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n","    image_dataset = image_dataset.map(lambda x: tf.io.read_file(x))\n","    image_dataset = image_dataset.map(lambda x: tf.image.decode_jpeg(x, channels=3))\n","    image_dataset = image_dataset.map(lambda x: preprocess_image(x, target_size))\n","\n","    lesions_tensor = tf.convert_to_tensor(lesions, dtype=tf.float32)\n","\n","    dataset = tf.data.Dataset.from_tensor_slices((image_dataset, lesions))\n","    dataset = dataset.shuffle(buffer_size=buffer_size)\n","    dataset = dataset.batch(batch_size)\n","\n","    return dataset"],"metadata":{"id":"qEuSRFdOTKoU","executionInfo":{"status":"ok","timestamp":1693536112030,"user_tz":-540,"elapsed":5,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sALkX2o9Syb_"},"source":["# camera cat"]},{"cell_type":"code","source":["lesions[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9jSpRrBgn_D-","executionInfo":{"status":"ok","timestamp":1693536239866,"user_tz":-540,"elapsed":5,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"7ee27200-447e-4a6e-e087-12aaf75e606a"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['A6', 'A6', 'A6', 'A6', 'A6', 'A6', 'A6', 'A6', 'A6', 'A6']"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":6312,"status":"error","timestamp":1693536120249,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"yH2t11ewBsJe","colab":{"base_uri":"https://localhost:8080/","height":408},"outputId":"7fd570a7-a6c8-4afa-e49f-0def5458e029"},"outputs":[{"output_type":"stream","name":"stderr","text":["Walking directories: 14 dir [00:01, 12.54 dir/s]\n","Loading JSON: 100%|██████████| 5500/5500 [00:05<00:00, 1098.55 file/s]\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-738590567397>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_and_json_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlesions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolygon_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_locations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_data_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlesions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-e24f81d1edff>\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(image_paths, lesions, buffer_size, batch_size, target_size)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mimage_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlesions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m     \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_tensor_slices_op.py\u001b[0m in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_from_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_TensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_tensor_slices_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid `element`. `element` should not be empty.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mto_batched_tensor_list\u001b[0;34m(element_spec, element)\u001b[0m\n\u001b[1;32m    383\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m   \u001b[0;31m# pylint: disable=g-long-lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m   return _to_tensor_list_helper(\n\u001b[0m\u001b[1;32m    386\u001b[0m       lambda state, spec, component: state + spec._to_batched_tensor_list(\n\u001b[1;32m    387\u001b[0m           component), element_spec, element)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m_to_tensor_list_helper\u001b[0;34m(encode_fn, element_spec, element)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencode_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m   return functools.reduce(\n\u001b[0m\u001b[1;32m    361\u001b[0m       reduce_fn, zip(nest.flatten(element_spec), nest.flatten(element)), [])\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mreduce_fn\u001b[0;34m(state, value)\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;34mf\"dtype {spec.dtype} and shape {spec.shape}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mencode_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m   return functools.reduce(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(state, spec, component)\u001b[0m\n\u001b[1;32m    384\u001b[0m   \u001b[0;31m# pylint: disable=g-long-lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m   return _to_tensor_list_helper(\n\u001b[0;32m--> 386\u001b[0;31m       lambda state, spec, component: state + spec._to_batched_tensor_list(\n\u001b[0m\u001b[1;32m    387\u001b[0m           component), element_spec, element)\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_to_batched_tensor_list\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4610\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4612\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Slicing dataset elements is not supported for rank 0.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4613\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Slicing dataset elements is not supported for rank 0."]}],"source":["src_path = '/content/drive/Shareddrives/반려묘/일반카메라'\n","num_files = 500\n","buffer_size = 1000\n","batch_size = 64\n","target_size = (96, 96)\n","\n","image_paths, json_paths = get_image_and_json_paths(src_path, num_files)\n","lesions, polygon_locations, box_locations = extract_data_from_json(json_paths)\n","\n","onehot_encoder = OneHotEncoder(sparse=False)\n","onehot_encoded = onehot_encoder.fit_transform(lesions.reshape(-1, 1))\n","dataset = create_dataset(image_paths, onehot_encoded, buffer_size, batch_size, target_size)"]},{"cell_type":"code","source":["df = pd.DataFrame({'image_path': image_paths, 'lesion': lesions})\n","\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"Kr-yYQQRlxFE","executionInfo":{"status":"ok","timestamp":1693467799720,"user_tz":-540,"elapsed":6,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"cdaa6b10-1f8f-4505-9818-b012899ccb8e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             image_path lesion\n","0     /content/drive/Shareddrives/반려묘/일반카메라/유증상...     A6\n","1     /content/drive/Shareddrives/반려묘/일반카메라/유증상...     A6\n","2     /content/drive/Shareddrives/반려묘/일반카메라/유증상...     A6\n","3     /content/drive/Shareddrives/반려묘/일반카메라/유증상...     A6\n","4     /content/drive/Shareddrives/반려묘/일반카메라/유증상...     A6\n","...                                                 ...    ...\n","5495  /content/drive/Shareddrives/반려묘/일반카메라/무증상...     A7\n","5496  /content/drive/Shareddrives/반려묘/일반카메라/무증상...     A7\n","5497  /content/drive/Shareddrives/반려묘/일반카메라/무증상...     A7\n","5498  /content/drive/Shareddrives/반려묘/일반카메라/무증상...     A7\n","5499  /content/drive/Shareddrives/반려묘/일반카메라/무증상...     A7\n","\n","[5500 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-13260dde-0ad5-41e3-b8e7-df4b4aeb1f4f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_path</th>\n","      <th>lesion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/유증상...</td>\n","      <td>A6</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/유증상...</td>\n","      <td>A6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/유증상...</td>\n","      <td>A6</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/유증상...</td>\n","      <td>A6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/유증상...</td>\n","      <td>A6</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5495</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/무증상...</td>\n","      <td>A7</td>\n","    </tr>\n","    <tr>\n","      <th>5496</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/무증상...</td>\n","      <td>A7</td>\n","    </tr>\n","    <tr>\n","      <th>5497</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/무증상...</td>\n","      <td>A7</td>\n","    </tr>\n","    <tr>\n","      <th>5498</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/무증상...</td>\n","      <td>A7</td>\n","    </tr>\n","    <tr>\n","      <th>5499</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/무증상...</td>\n","      <td>A7</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5500 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13260dde-0ad5-41e3-b8e7-df4b4aeb1f4f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-13260dde-0ad5-41e3-b8e7-df4b4aeb1f4f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-13260dde-0ad5-41e3-b8e7-df4b4aeb1f4f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-310ff465-05be-463a-83c0-6b42fc48b037\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-310ff465-05be-463a-83c0-6b42fc48b037')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const charts = await google.colab.kernel.invokeFunction(\n","          'suggestCharts', [key], {});\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-310ff465-05be-463a-83c0-6b42fc48b037 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["datagen = ImageDataGenerator(rescale=1.0/255)"],"metadata":{"id":"rCDslFuU79Ds"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"],"metadata":{"id":"O7m1-ZfmV-01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_generator = datagen.flow_from_dataframe(\n","    dataframe=train_df,\n","    x_col='image_path',\n","    y_col='lesion',\n","    class_mode='categorical',\n","    target_size=(96, 96),\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p1IU9hs473GM","executionInfo":{"status":"ok","timestamp":1693467800438,"user_tz":-540,"elapsed":722,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"869ed91a-f802-49d5-a0ae-09a3beca38eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 4400 validated image filenames belonging to 4 classes.\n"]}]},{"cell_type":"code","source":["val_generator = datagen.flow_from_dataframe(\n","    dataframe=val_df,\n","    x_col='image_path',\n","    y_col='lesion',\n","    class_mode='categorical',\n","    target_size=(96, 96),\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hS9LF1g3U3cu","executionInfo":{"status":"ok","timestamp":1693467800439,"user_tz":-540,"elapsed":5,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"5a4b2e71-7e46-4d60-f1c5-b105a0ea57ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 550 validated image filenames belonging to 4 classes.\n"]}]},{"cell_type":"code","source":["test_generator = datagen.flow_from_dataframe(\n","    dataframe=test_df,\n","    x_col='image_path',\n","    y_col='lesion',\n","    class_mode='categorical',\n","    target_size=(96, 96),\n","    shuffle=False\n",")"],"metadata":{"id":"mXLi75CKV3Mk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693467800439,"user_tz":-540,"elapsed":3,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"61784478-92ea-40ae-99b9-7603ac1759a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 550 validated image filenames belonging to 4 classes.\n"]}]},{"cell_type":"code","source":["model = models.Sequential()\n","\n","model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(96,96,3)))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(3, 3)))\n","\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model.add(Dropout(0.25))\n","\n","model.add(Flatten())\n","\n","model.add(Dense(units=1024, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.5))\n","\n","model.add(Dense(units=4, activation='softmax'))\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n","\n","opt = Adam(learning_rate=0.001)\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])"],"metadata":{"id":"JGUbxOe3TxF0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history1 = model.fit(\n","    train_generator,\n","    epochs=150,\n","    batch_size=64,\n","    callbacks=[early_stopping]\n",")"],"metadata":{"id":"2iSRx9X0aFUi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = model.predict(test_generator)\n","y_pred_classes = np.argmax(y_pred, axis=1)\n","y_true_classes = test_generator.classes\n","\n","print(classification_report(y_true_classes, y_pred_classes))"],"metadata":{"id":"_QCLXIZcTyct"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TensorFlow Lite 모델로 변환\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","tflite_model = converter.convert()\n","\n","# 변환된 모델을 파일로 저장\n","with open('72.tflite', 'wb') as f:\n","    f.write(tflite_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7f5CnkuvE2g","executionInfo":{"status":"ok","timestamp":1693202560336,"user_tz":-540,"elapsed":7443,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"7183ee6a-4113-42c6-96b5-d1e77feedd93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"]}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from skimage.feature import greycomatrix\n","import h5py\n","\n","class ImageFeatureExtractor:\n","    def __init__(self, target_size=(96, 96)):\n","        self.target_size = target_size\n","\n","    def preprocess_image(self, image_path):\n","        image = cv2.imread(image_path)\n","        image = cv2.resize(image, self.target_size)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        return image\n","\n","    def extract_color_histogram(self, image):\n","        histogram = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n","        histogram = cv2.normalize(histogram, histogram).flatten()\n","        return histogram\n","\n","    def extract_hu_moments(self, image):\n","        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n","        moments = cv2.HuMoments(cv2.moments(gray_image)).flatten()\n","        return moments\n","\n","    def extract_haralick_texture(self, image):\n","        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n","        texture = greycomatrix(gray_image, distances=[1], angles=[0], symmetric=True, normed=True)\n","        haralick_features = np.mean(texture, axis=(0, 1, 2))\n","        return haralick_features\n","\n","    def extract_features(self, image_path):\n","        image = self.preprocess_image(image_path)\n","        color_histogram = self.extract_color_histogram(image)\n","        hu_moments = self.extract_hu_moments(image)\n","        haralick_texture = self.extract_haralick_texture(image)\n","        global_features = np.concatenate([color_histogram, hu_moments, haralick_texture])\n","        return global_features\n","\n","    def save_features_to_hdf5(self, features, lesions, output_path):\n","        with h5py.File(output_path, 'w') as f:\n","            f.create_dataset('features', data=features)\n","            f.create_dataset('lesions', data=lesions)\n","\n","# HDF5 파일로부터 데이터 읽어오기\n","def load_data_from_hdf5(file_path):\n","    with h5py.File(file_path, 'r') as f:\n","        features = f['features'][:]\n","        lesions = f['lesions'][:]\n","    return features, lesions"],"metadata":{"id":"2CiVGFj7bjdH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_extractor = ImageFeatureExtractor(target_size=(96, 96))\n","\n","features = []\n","for image_path in image_paths:\n","    image_features = feature_extractor.extract_features(image_path)\n","    features.append(image_features)\n","\n","features = np.array(features)\n","lesions = np.array(lesions)\n","\n","output_path = 'features.h5'\n","feature_extractor.save_features_to_hdf5(features, lesions, output_path)"],"metadata":{"id":"maXZ70DWYZwN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hdf5_file_path = 'path_to_your_hdf5_file.hdf5'\n","\n","features, lesions = load_data_from_hdf5(hdf5_file_path)\n","\n","X_train, X_test, y_train, y_test = train_test_split(features, lesions, test_size=0.2, random_state=42)\n","X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"],"metadata":{"id":"EeLaIH1wZTsL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history2 = model.fit(X_train, X_test, epochs=150, batch_size=32, validation_data=(X_val, y_val))"],"metadata":{"id":"ig6GvlEyYaqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = model.predict(X_test)\n","y_pred_classes = np.argmax(y_pred, axis=1)\n","y_true_classes = np.argmax(y_test, axis=1)\n","\n","print(classification_report(y_true_classes, y_pred_classes))"],"metadata":{"id":"tm9LepXEYalM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qqg4JHS0Y_yH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def combine_images_and_masks(image_list, mask_list):\n","    combined_images = []\n","\n","    for i in tqdm(range(len(image_list)), desc='Combining Images and Masks'):\n","        image = image_list[i]\n","        mask = mask_list[i]\n","\n","        mask_channel = np.zeros_like(image[:, :, 0], dtype=np.uint8)\n","\n","        mask_channel[mask == 1] = 255\n","\n","        combined_image = np.dstack((image, mask_channel))\n","\n","        combined_images.append(combined_image)\n","\n","    return combined_images\n","\n","def one_hot_encode_labels(labels):\n","    label_binarizer = LabelBinarizer()\n","    encoded_labels = label_binarizer.fit_transform(labels)\n","    return np.array(encoded_labels), label_binarizer\n","\n","def create_polygon_binary_masks(image_list, polygon_locations_list):\n","    binary_masks = []\n","\n","    for image, polygon_locations in tqdm(zip(image_list, polygon_locations_list), desc='Generating Binary Masks', total=len(image_list)):\n","        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n","\n","        for poly_coords in polygon_locations:\n","            if poly_coords:\n","                poly_points = []\n","                i = 1\n","                while f'x{i}' in poly_coords and f'y{i}' in poly_coords:\n","                    x = poly_coords[f'x{i}']\n","                    y = poly_coords[f'y{i}']\n","                    poly_points.append([x, y])\n","                    i += 1\n","                if len(poly_points) > 0:\n","                    poly_points = np.array(poly_points, dtype=np.int32)\n","                    cv2.fillPoly(mask, [poly_points], 255)\n","            print(poly_coords)\n","\n","        binary_masks.append(mask)\n","\n","    return binary_masks\n","\n","def visualize_masks(image_data, mask_data, selected_indices, figsize=(10, 5)):\n","    for idx in selected_indices:\n","        image = image_data[idx]\n","        mask_map = mask_data[idx]\n","\n","        plt.figure(figsize=figsize)\n","        plt.subplot(1, mask_map.shape[2] + 1, 1)\n","        plt.imshow(image)\n","        plt.title('Original Image')\n","\n","        for ch in range(mask_map.shape[2]):\n","            plt.subplot(1, mask_map.shape[2] + 1, ch + 2)\n","            plt.imshow(mask_map[:, :, ch], cmap='gray')\n","            plt.title(f'Channel {ch}')\n","\n","        plt.tight_layout()\n","        plt.show()"],"metadata":{"id":"5BhTlrhC2uf8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# microscope cat"],"metadata":{"id":"5St-XgMdS3A_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZOVWv80Sea6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EG-vxlAj_HYD"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f0qJBmnqSbr7"},"outputs":[],"source":["from tensorflow.keras.layers import PReLU\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import MeanSquaredError\n","from tensorflow.keras.layers import Concatenate, MaxPooling2D, BatchNormalization\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.layers import concatenate\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.losses import mean_squared_error\n","\n","def inception_module(input_layer, filters):\n","    conv1x1 = Conv2D(filters[0], (1, 1), activation='relu')(input_layer)\n","    conv3x3_reduce = Conv2D(filters[1], (1, 1), activation='relu')(input_layer)\n","    conv3x3 = Conv2D(filters[2], (3, 3), padding='same', activation='relu')(conv3x3_reduce)\n","    conv5x5_reduce = Conv2D(filters[3], (1, 1), activation='relu')(input_layer)\n","    conv5x5 = Conv2D(filters[4], (5, 5), padding='same', activation='relu')(conv5x5_reduce)\n","    maxpool = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_layer)\n","    maxpool_conv = Conv2D(filters[5], (1, 1), activation='relu')(maxpool)\n","    inception_output = Concatenate(axis=-1)([conv1x1, conv3x3, conv5x5, maxpool_conv])\n","    return inception_output\n","\n","# Input\n","input_shape = (128, 128, 3)\n","input_layer = Input(shape=input_shape)\n","\n","# Inception block\n","inception_output = inception_module(input_layer, filters=[64, 128, 192, 32, 96, 64])\n","inception_output = inception_module(inception_output, filters=[64, 128, 192, 32, 96, 64])\n","# Add more inception modules if needed\n","\n","# Primary Capsule layer\n","primary_capsules = Conv2D(32, (1, 1), activation='relu')(inception_output)\n","\n","# Higher Capsule layers\n","# (Add imperative routing mechanism layers here)\n","\n","# PReLU activation for routing\n","higher_capsules_prelu = PReLU()(higher_capsules)\n","\n","# Flatten and Fully Connected layers\n","capsule_flatten = Flatten()(higher_capsules_prelu)  # Flatten higher capsules\n","output_layer = Dense(2, activation='softmax')(capsule_flatten)  # Two capsules: parasitized and uninfected\n","\n","# Create the model\n","model = Model(inputs=input_layer, outputs=output_layer)\n","\n","# Compile the model with Adam optimizer and custom loss function\n","optimizer = Adam(learning_rate=0.007, beta_1=0.8)\n","loss_fn = custom_loss_function # Define the custom loss function as described in the paper\n","model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n","\n","# Print the model summary\n","model.summary()"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"gpuType":"A100","mount_file_id":"1Dnsl078JzGwH-snRqYpe4v_RDBNGZeJb","authorship_tag":"ABX9TyMijE2fjmeoaGyXC+j+Mf0j"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}