{"cells":[{"cell_type":"markdown","metadata":{"id":"SoOMF4kHSwMS"},"source":["# Skin lesion classification of dermoscopic images using machine learning and convolutional neural network\n","\n","19 December 2022\n","\n","https://www.nature.com/articles/s41598-022-22644-9#Tab7\n","\n","https://aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=realm&dataSetSn=561"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"EzLpCjbDOZy_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692340949464,"user_tz":-540,"elapsed":2936,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"2513e555-eba2-4b43-fb85-15d17b8f7e0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"sALkX2o9Syb_"},"source":["# Preprocessing"]},{"cell_type":"code","source":["import cv2\n","import glob\n","import json\n","import numpy as np\n","import os\n","from tqdm import tqdm\n","import re\n","import pandas as pd"],"metadata":{"id":"fRfHAxbnUa_b","executionInfo":{"status":"ok","timestamp":1692340949465,"user_tz":-540,"elapsed":4,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","execution_count":25,"metadata":{"id":"i7jwMMug2YXB","executionInfo":{"status":"ok","timestamp":1692342482319,"user_tz":-540,"elapsed":652,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"outputs":[],"source":["def get_image_and_json_paths(src_path):\n","    image_paths = glob.glob(os.path.join(src_path, '**', '*.jpg'), recursive=True)\n","    image_paths.sort()\n","\n","    json_paths = glob.glob(os.path.join(src_path, '**', '*.json'), recursive=True)\n","    json_paths.sort()\n","\n","    return image_paths, json_paths\n","\n","\n","def get_image_data(image_paths):\n","    image_data = []\n","    for image_path in tqdm(image_paths, desc='Loading Images'):\n","        image = cv2.imread(image_path)\n","        image_data.append(image)\n","\n","    return image_data\n","\n","def extract_metadata_and_locations_from_json(json_paths):\n","    meta_data = []\n","    polygon_data = []\n","    box_data = []\n","\n","    for json_path in tqdm(json_paths, desc='Loading JSON', unit=' file'):\n","        with open(json_path, \"r\", encoding=\"utf-8\") as file:\n","            file_content = file.read()\n","            control_char_regex = r'[\\x00-\\x1F\\x7F-\\x9F]'\n","            cleaned_content = re.sub(control_char_regex, '', file_content)\n","            json_data = json.loads(cleaned_content)\n","            metadata = json_data.get('metaData', None)\n","\n","            filtered_metadata = {\n","                'breed': metadata.get('breed', None),\n","                'age': metadata.get('age', None),\n","                'gender': metadata.get('gender', None),\n","                'region': metadata.get('region', None),\n","                'lesions': metadata.get('lesions', None),\n","                'polygon_location': [],\n","                'box_location': []\n","            }\n","\n","            labeling_info = json_data.get('labelingInfo', [])\n","\n","            for entry in labeling_info:\n","                if 'polygon' in entry:\n","                    polygon_metadata = filtered_metadata.copy()\n","                    polygon_metadata['polygon_location'] = entry['polygon'].get('location', None)\n","                    polygon_data.append(polygon_metadata)\n","                if 'box' in entry:\n","                    box_metadata = filtered_metadata.copy()\n","                    box_metadata['box_location'] = entry['box'].get('location', None)\n","                    box_data.append(box_metadata)\n","\n","            meta_data.append(filtered_metadata)\n","\n","    return meta_data, polygon_data, box_data\n","\n","def create_mask_maps(image_data, polygon_data, box_data):\n","    mask_maps = []\n","\n","    for image, polygons, boxes in tqdm(zip(image_data, polygon_data, box_data), desc='Generating Mask Maps'):\n","        mask_map = np.zeros(image.shape[:2] + (4,), dtype=np.uint8)\n","\n","        for entry in tqdm(polygon_data, desc='Drawing Polygon'):\n","            loc = entry['polygon_location']\n","            fill_value = 1  # 수정된 부분\n","            if loc:\n","                polygon_points = np.array(loc, np.int32).reshape((-1, 1, 2))\n","                cv2.fillPoly(mask_map[..., 0], [polygon_points], 255)\n","                cv2.fillPoly(mask_map[..., 1], [polygon_points], fill_value)\n","\n","        for entry in tqdm(box_data, desc='Drawing Box'):\n","            loc = entry['box_location']\n","            fill_value = 1  # 수정된 부분\n","            if loc:\n","                cv2.rectangle(mask_map[..., 2], tuple(loc[:2]), tuple(loc[2:]), 255, thickness=-1)\n","                cv2.rectangle(mask_map[..., 3], tuple(loc[:2]), tuple(loc[2:]), fill_value, thickness=-1)\n","\n","        mask_maps.append(mask_map)\n","\n","    return mask_maps\n","\n","def resize_images(images, width, height):\n","    resized_images = []\n","\n","    for image in tqdm(images, desc='Resizing Images'):\n","        resized_image = cv2.resize(image, (width, height), interpolation=cv2.INTER_AREA)\n","        resized_images.append(resized_image)\n","\n","    return resized_images\n","\n","def prepare_image_data_and_masks(src_path):\n","    image_paths, json_paths = get_image_and_json_paths(src_path)\n","    image_data = get_image_data(image_paths)\n","    meta_data, polygon_data, box_data = extract_metadata_and_locations_from_json(json_paths)\n","    masks_maps = create_mask_maps(image_data, polygon_data, box_data)\n","    original_size_mask_maps = resize_images(masks_maps, 96, 96)\n","    resized_mask_maps = create_mask_maps(resize_images(image_data, 96, 96), polygon_data, box_data)\n","\n","    metadata_df = pd.DataFrame(meta_data)\n","\n","    return image_data, metadata_df, original_size_mask_maps, resized_mask_maps"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"yH2t11ewBsJe","executionInfo":{"status":"error","timestamp":1692342510942,"user_tz":-540,"elapsed":26970,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"colab":{"base_uri":"https://localhost:8080/","height":463},"outputId":"8f233954-c5b7-4b6c-e1bb-45070439f634"},"outputs":[{"output_type":"stream","name":"stderr","text":["Loading Images: 100%|██████████| 750/750 [00:25<00:00, 28.91it/s]\n","Loading JSON: 100%|██████████| 749/749 [00:00<00:00, 952.63 file/s]\n","Generating Mask Maps: 0it [00:00, ?it/s]\n","Drawing Polygon:   0%|          | 0/764 [00:00<?, ?it/s]\n","Generating Mask Maps: 0it [00:00, ?it/s]\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-4eb19c65abab>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msrc_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/Shareddrives/반려견\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_size_mask_maps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresized_mask_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_image_data_and_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-25-91c584248a99>\u001b[0m in \u001b[0;36mprepare_image_data_and_masks\u001b[0;34m(src_path)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mimage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mmeta_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolygon_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_metadata_and_locations_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mmasks_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_mask_maps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolygon_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0moriginal_size_mask_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks_maps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mresized_mask_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_mask_maps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresize_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolygon_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-91c584248a99>\u001b[0m in \u001b[0;36mcreate_mask_maps\u001b[0;34m(image_data, polygon_data, box_data)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolygon_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Drawing Polygon'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'polygon_location'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mfill_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lesions'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mpolygon_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"]}],"source":["src_path = \"/content/drive/Shareddrives/반려견\"\n","image_data, metadata_df, original_size_mask_maps, resized_mask_maps = prepare_image_data_and_masks(src_path)"]},{"cell_type":"markdown","metadata":{"id":"auDavqpSS5rc"},"source":["# Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9KN24A9E1JFc"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import models\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4QcwK9UBb2u"},"outputs":[],"source":["# train, test set 분할\n","test_size = 0.2\n","temp_img_data, test_img_data, temp_meta_data, test_meta_data, temp_orig_size_maps, test_orig_size_maps, temp_resized_maps, test_resized_maps = train_test_split(\n","    image_data, metadata_df, original_size_mask_maps, resized_mask_maps,\n","    test_size=test_size, random_state=42\n",")\n","\n","# test set을 기준으로 적용된 비율로 val_size 계산\n","val_size = 0.2\n","adjusted_val_size = val_size / (1.0 - test_size)\n","\n","# train, validation set 분할\n","train_img_data, val_img_data, train_meta_data, val_meta_data, train_orig_size_maps, val_orig_size_maps, train_resized_maps, val_resized_maps = train_test_split(\n","    temp_img_data, temp_meta_data, temp_orig_size_maps, temp_resized_maps,\n","    test_size=adjusted_val_size, random_state=42\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZOVWv80Sea6"},"outputs":[],"source":["# Sequential 모델 생성\n","model = models.Sequential()\n","\n","# 첫번째 Conv2D 레이어\n","model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(96,96,3)))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(3, 3)))\n","\n","# 첫번째 Dropout 레이어\n","model.add(Dropout(0.25))\n","\n","# 두번째 Conv2D 레이어\n","model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# 두번째 Dropout 레이어\n","model.add(Dropout(0.25))\n","\n","# 세번째 Conv2D 레이어\n","model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# 세번째 Dropout 레이어\n","model.add(Dropout(0.25))\n","\n","# Flatten 레이어\n","model.add(Flatten())\n","\n","# 첫번째 Dense 레이어\n","model.add(Dense(units=1024, activation='relu'))\n","model.add(BatchNormalization())\n","\n","# 두번째 Dropout 레이어\n","model.add(Dropout(0.5))\n","\n","# 두번째 Dense 레이어: 최종 출력 레이어\n","model.add(Dense(units=7, activation='softmax'))\n","\n","# 모델 컴파일\n","opt = Adam(lr=0.001, decay=0.00001)\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","# 모델 구조 요약\n","model.summary()\n","\n","# 모델 학습\n","epochs = 150\n","batch_size = 32\n","\n","history = model.fit(train_data, epochs=epochs, batch_size=batch_size, validation_data=val_data)\n","\n","# 모델 평가\n","test_loss, test_acc = model.evaluate(test_data)\n","print(\"Test Loss:\", test_loss)\n","print(\"Test Accuracy:\", test_acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EG-vxlAj_HYD"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f0qJBmnqSbr7"},"outputs":[],"source":["# TensorFlow Lite 모델로 변환\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","tflite_model = converter.convert()\n","\n","# 변환된 모델을 파일로 저장\n","with open('your_model.tflite', 'wb') as f:\n","    f.write(tflite_model)"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyMLw3TN8WHaJDJVepUgoCFM"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}