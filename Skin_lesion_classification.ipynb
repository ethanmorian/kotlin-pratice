{"cells":[{"cell_type":"markdown","metadata":{"id":"SoOMF4kHSwMS"},"source":["# Skin lesion classification of dermoscopic images using machine learning and convolutional neural network\n","\n","19 December 2022\n","\n","https://www.nature.com/articles/s41598-022-22644-9#Tab7\n","\n","https://aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=realm&dataSetSn=561"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2647,"status":"ok","timestamp":1695094551794,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"EzLpCjbDOZy_","outputId":"445fb993-d65b-4084-e369-09023b034fb6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"491U9sI3TVVj","executionInfo":{"status":"ok","timestamp":1695094555235,"user_tz":-540,"elapsed":3442,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"outputs":[],"source":["import os\n","import glob\n","import cv2\n","import numpy as np\n","from tqdm import tqdm\n","\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n","from sklearn.preprocessing import LabelEncoder\n","\n","from skimage.feature import graycomatrix\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Concatenate"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"qEuSRFdOTKoU","executionInfo":{"status":"ok","timestamp":1695094555235,"user_tz":-540,"elapsed":4,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"outputs":[],"source":["def get_path_and_label(src_path, num_files_per_folder):\n","    image_paths = []\n","    lesions = []\n","\n","    for root, dirs, files in tqdm(os.walk(src_path), desc='Walking directories', unit=' dir'):\n","        for dir in dirs:\n","            dir_path = os.path.join(root, dir)\n","            image_files = sorted(glob.glob(os.path.join(dir_path, '*.jpg')))\n","\n","            for image_file in image_files[:num_files_per_folder]:\n","                filename = os.path.basename(image_file)\n","                parts = filename.split('_')\n","                if len(parts) >= 3:\n","                    second_part = parts[2]\n","                    lesions.append(second_part)\n","                    image_paths.append(image_file)\n","\n","    return image_paths, lesions\n","\n","def preprocess_images(image_paths):\n","    images = []\n","    gray_images = []\n","\n","    for image_path in tqdm(image_paths, desc='Preprocessing Images'):\n","        image = cv2.imread(image_path)\n","        if image is None:\n","            print(f\"Error loading image: {image_path}\")\n","            continue\n","        image = cv2.resize(image, target_size)\n","\n","        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        gray_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2GRAY)\n","\n","        images.append(rgb_image)\n","        gray_images.append(gray_image)\n","\n","    return np.array(images), np.array(gray_images)\n","\n","def extract_color_histograms(images):\n","    histograms = np.zeros((images.shape[0], 512))\n","\n","    for i, image in tqdm(enumerate(images), desc='Extracting Color Histograms'):\n","        histogram = cv2.calcHist([image], [0, 1, 2], None,[8 ,8 ,8 ], [0 ,256 ,0 ,256 ,0 ,256])\n","        histogram = cv2.normalize(histogram,histogram).flatten()\n","\n","        histograms[i] = histogram\n","\n","    return histograms\n","\n","def extract_hu_moments(gray_images):\n","    moments_list = [cv2.HuMoments(cv2.moments(gray_image)).flatten() for gray_image in tqdm(gray_images, desc='Extracting Hu Moments')]\n","    return np.array(moments_list)\n","\n","def extract_haralick_textures(gray_images):\n","    textures_list = [np.mean(graycomatrix(gray_image, distances=[1], angles=[0], symmetric=True, normed=True), axis=(0, 1)).flatten() for gray_image in tqdm(gray_images, desc='Extracting Haralick Textures')]\n","    return np.array(textures_list)\n","\n","def extract_features(image_paths):\n","    images, gray_images = preprocess_images(image_paths)\n","    color_histograms = extract_color_histograms(images)\n","    hu_moments = extract_hu_moments(gray_images)\n","    haralick_textures = extract_haralick_textures(gray_images)\n","\n","    feature_vectors = np.concatenate([color_histograms,\n","                                      hu_moments,\n","                                      haralick_textures], axis=1)\n","\n","    return feature_vectors\n","\n","def create_dataset(image_paths, image_feature_vectors, lesions):\n","    encoder = LabelEncoder()\n","    encoded_labels = encoder.fit_transform(lesions)\n","\n","    num_classes = len(encoder.classes_)\n","    one_hot_labels = tf.one_hot(encoded_labels, depth=num_classes)\n","\n","    dataset = tf.data.Dataset.from_tensor_slices(({\"image_input\": image_paths,\n","                                                   \"global_feature_input\": image_feature_vectors},\n","                                                  one_hot_labels))\n","\n","    def load_and_preprocess(inputs_dict, label):\n","        img_raw = tf.io.read_file(inputs_dict[\"image_input\"])\n","        img_tensor = tf.image.decode_jpeg(img_raw, channels=3)\n","        img = tf.image.resize(img_tensor, target_size)\n","        img = img / 255.0\n","\n","        return {\"image_input\": img,\n","                \"global_feature_input\": inputs_dict[\"global_feature_input\"]}, label\n","\n","    dataset = dataset.map(load_and_preprocess)\n","\n","    return dataset"]},{"cell_type":"markdown","metadata":{"id":"sALkX2o9Syb_"},"source":["# camera cat"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yH2t11ewBsJe","executionInfo":{"status":"ok","timestamp":1695094568319,"user_tz":-540,"elapsed":13088,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"c467fba3-3d8f-422a-8728-c83525dfd3c7"},"outputs":[{"output_type":"stream","name":"stderr","text":["Walking directories: 14 dir [00:12,  1.08 dir/s]\n"]}],"source":["src_path = '/content/drive/Shareddrives/반려묘/일반카메라/Training'\n","num_files_per_folder = 2000\n","\n","image_paths, lesions = get_path_and_label(src_path, num_files_per_folder)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AP9hy-9_CyBe","outputId":"1e073a4c-5eae-41a0-8c25-08b01c693026"},"outputs":[{"output_type":"stream","name":"stderr","text":["Preprocessing Images:  16%|█▌        | 3244/20490 [01:20<04:25, 64.96it/s]"]}],"source":["target_size = (256, 256)\n","\n","image_feature_vectors = extract_features(image_paths)"]},{"cell_type":"code","source":["print('Number of image paths: ', len(image_paths))\n","print('Number of image feature vectors: ', len(image_feature_vectors))\n","print('Number of labels: ', len(lesions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JvahrmpVUEjx","executionInfo":{"status":"ok","timestamp":1695109658683,"user_tz":-540,"elapsed":382,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"29e60070-b9c1-40c7-c640-a3d928857583"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of image paths:  20490\n","Number of image feature vectors:  20490\n","Number of labels:  20490\n"]}]},{"cell_type":"code","source":["print('image paths:\\n', image_paths[:5])\n","print('image feature vectors:\\n', image_feature_vectors[:5])\n","print('labels:\\n', lesions[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ly6Jqn2X5-CD","executionInfo":{"status":"ok","timestamp":1695109295710,"user_tz":-540,"elapsed":362,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"b94346c3-2d51-4192-c429-6ec50cba3ddc"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["image paths:\n"," ['/content/drive/Shareddrives/반려묘/일반카메라/Training/유증상/A6_결절_종괴_잔여/IMG_C_A6_807019.jpg', '/content/drive/Shareddrives/반려묘/일반카메라/Training/유증상/A6_결절_종괴_잔여/IMG_C_A6_807020.jpg', '/content/drive/Shareddrives/반려묘/일반카메라/Training/유증상/A6_결절_종괴_잔여/IMG_C_A6_807021.jpg', '/content/drive/Shareddrives/반려묘/일반카메라/Training/유증상/A6_결절_종괴_잔여/IMG_C_A6_807022.jpg', '/content/drive/Shareddrives/반려묘/일반카메라/Training/유증상/A6_결절_종괴_잔여/IMG_C_A6_807023.jpg']\n","image feature vectors:\n"," [[ 3.81492637e-02  2.71042722e-04  0.00000000e+00 ... -3.34269860e-16\n","  -3.60531132e-23  1.52587891e-05]\n"," [ 6.17622733e-02  4.69318184e-05  0.00000000e+00 ...  3.94745778e-16\n","  -4.53436791e-23  1.52587891e-05]\n"," [ 2.59372950e-01  2.52708830e-02  0.00000000e+00 ...  6.14236736e-14\n","   1.20199478e-20  1.52587891e-05]\n"," [ 3.76893729e-02  2.93792435e-03  0.00000000e+00 ... -7.87975735e-17\n","  -3.94002962e-24  1.52587891e-05]\n"," [ 2.54094005e-02  0.00000000e+00  0.00000000e+00 ... -4.20022978e-16\n","  -6.81472638e-23  1.52587891e-05]]\n","labels:\n"," ['A6', 'A6', 'A6', 'A6', 'A6']\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"id":"yJ9Te2aqmIuB","executionInfo":{"status":"ok","timestamp":1695109296236,"user_tz":-540,"elapsed":2,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"outputs":[],"source":["buffer_size = 1000\n","batch_size = 64\n","\n","dataset = create_dataset(image_paths, image_feature_vectors, lesions)\n","dataset = dataset.shuffle(buffer_size).batch(batch_size)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"23mCdag7SVAd","executionInfo":{"status":"ok","timestamp":1695109298152,"user_tz":-540,"elapsed":378,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"outputs":[],"source":["dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n","\n","train_ratio = 0.8\n","val_ratio = 0.1\n","test_ratio = 0.1\n","\n","train_size = int(train_ratio * dataset_size)\n","val_size = int(val_ratio * dataset_size)\n","test_size = int(test_ratio * dataset_size)\n","\n","train_dataset = dataset.take(train_size)\n","val_dataset_temp = dataset.skip(train_size)\n","\n","val_dataset = val_dataset_temp.take(val_size)\n","test_dataset = val_dataset_temp.skip(val_size)"]},{"cell_type":"code","source":["print('Expected train_dataset size: ', train_size)\n","print('Expected val_dataset size: ', val_size)\n","print('Expected test_dataset size: ', test_size)\n","\n","print('Actual train_dataset size: ', tf.data.experimental.cardinality(train_dataset).numpy())\n","print('Actual val_dataset size: ', tf.data.experimental.cardinality(val_dataset).numpy())\n","print('Actual test_dataset size: ', tf.data.experimental.cardinality(test_dataset).numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1wDcy8gyUZuu","executionInfo":{"status":"ok","timestamp":1695109328609,"user_tz":-540,"elapsed":540,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"79daa52d-96f1-4135-e5a7-5c7a597b7b11"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Expected train_dataset size:  256\n","Expected val_dataset size:  32\n","Expected test_dataset size:  32\n","Actual train_dataset size:  256\n","Actual val_dataset size:  32\n","Actual test_dataset size:  33\n"]}]},{"cell_type":"code","execution_count":16,"metadata":{"id":"JGUbxOe3TxF0","executionInfo":{"status":"ok","timestamp":1695109329106,"user_tz":-540,"elapsed":499,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"outputs":[],"source":["image_input = Input(shape=(256, 256, 3), name='image_input')\n","x = Conv2D(32, (3, 3), padding='same', activation='relu')(image_input)\n","x = BatchNormalization()(x)\n","x = MaxPooling2D(pool_size=(3 , 3))(x)\n","\n","x = Dropout(0.25)(x)\n","\n","x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n","x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n","x = BatchNormalization()(x)\n","x = MaxPooling2D(pool_size=(2 , 2))(x)\n","\n","x = Dropout(0.25)(x)\n","\n","x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n","x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n","x = BatchNormalization()(x)\n","x = MaxPooling2D(pool_size=(2 , 2))(x)\n","\n","x = Dropout(0.25)(x)\n","\n","global_feature_input = Input(shape=(520,), name='global_feature_input')\n","\n","combined_features= Concatenate()([Flatten()(x), global_feature_input])\n","\n","combined_features= Dense(1024, activation='relu')(combined_features)\n","combined_features= BatchNormalization ()(combined_features)\n","combined_features= Dropout(0.5)(combined_features)\n","\n","outputs= Dense(4, activation='softmax')(combined_features)\n","\n","model= Model(inputs=[image_input, global_feature_input], outputs=outputs)\n","\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"2iSRx9X0aFUi","executionInfo":{"status":"error","timestamp":1695109456503,"user_tz":-540,"elapsed":127400,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"colab":{"base_uri":"https://localhost:8080/","height":646},"outputId":"41680e50-fc67-4f89-8d3b-17dd1b2f1672"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/150\n","219/256 [========================>.....] - ETA: 19s - loss: 1.5570 - accuracy: 0.5083"]},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-a274e7d75fef>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  jpeg::Uncompress failed. Invalid JPEG data or crop window.\n\t [[{{node DecodeJpeg}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_9]]\n  (1) INVALID_ARGUMENT:  jpeg::Uncompress failed. Invalid JPEG data or crop window.\n\t [[{{node DecodeJpeg}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_3204]"]}],"source":["history = model.fit(\n","    train_dataset,\n","    validation_data=val_dataset,\n","    epochs=150,\n","    batch_size=64\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_QCLXIZcTyct","executionInfo":{"status":"aborted","timestamp":1695109456506,"user_tz":-540,"elapsed":12,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"outputs":[],"source":["y_true = np.concatenate([y.numpy() for x, y in test_dataset], axis=0)\n","y_pred = model.predict(test_dataset)\n","y_pred_classes = np.argmax(y_pred, axis=1)\n","y_true_classes = np.argmax(y_true, axis=1)\n","\n","print(classification_report(y_true_classes, y_pred_classes))\n","\n","print(confusion_matrix(y_true_classes, y_pred_classes))\n","\n","roc_auc = roc_auc_score(y_true, y_pred , multi_class='ovr')\n","print('ROC-AUC score:', roc_auc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s7f5CnkuvE2g","executionInfo":{"status":"aborted","timestamp":1695094543912,"user_tz":-540,"elapsed":12,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"outputs":[],"source":["# TensorFlow Lite 모델로 변환\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","tflite_model = converter.convert()\n","\n","# 변환된 모델을 파일로 저장\n","with open('3000_256.tflite', 'wb') as f:\n","    f.write(tflite_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EeLaIH1wZTsL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qqg4JHS0Y_yH"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"5St-XgMdS3A_"},"source":["# microscope cat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZOVWv80Sea6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EG-vxlAj_HYD"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f0qJBmnqSbr7"},"outputs":[],"source":["from tensorflow.keras.layers import PReLU\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import MeanSquaredError\n","from tensorflow.keras.layers import Concatenate, MaxPooling2D, BatchNormalization\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.layers import concatenate\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.losses import mean_squared_error\n","\n","def inception_module(input_layer, filters):\n","    conv1x1 = Conv2D(filters[0], (1, 1), activation='relu')(input_layer)\n","    conv3x3_reduce = Conv2D(filters[1], (1, 1), activation='relu')(input_layer)\n","    conv3x3 = Conv2D(filters[2], (3, 3), padding='same', activation='relu')(conv3x3_reduce)\n","    conv5x5_reduce = Conv2D(filters[3], (1, 1), activation='relu')(input_layer)\n","    conv5x5 = Conv2D(filters[4], (5, 5), padding='same', activation='relu')(conv5x5_reduce)\n","    maxpool = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_layer)\n","    maxpool_conv = Conv2D(filters[5], (1, 1), activation='relu')(maxpool)\n","    inception_output = Concatenate(axis=-1)([conv1x1, conv3x3, conv5x5, maxpool_conv])\n","    return inception_output\n","\n","# Input\n","input_shape = (128, 128, 3)\n","input_layer = Input(shape=input_shape)\n","\n","# Inception block\n","inception_output = inception_module(input_layer, filters=[64, 128, 192, 32, 96, 64])\n","inception_output = inception_module(inception_output, filters=[64, 128, 192, 32, 96, 64])\n","# Add more inception modules if needed\n","\n","# Primary Capsule layer\n","primary_capsules = Conv2D(32, (1, 1), activation='relu')(inception_output)\n","\n","# Higher Capsule layers\n","# (Add imperative routing mechanism layers here)\n","\n","# PReLU activation for routing\n","higher_capsules_prelu = PReLU()(higher_capsules)\n","\n","# Flatten and Fully Connected layers\n","capsule_flatten = Flatten()(higher_capsules_prelu)  # Flatten higher capsules\n","output_layer = Dense(2, activation='softmax')(capsule_flatten)  # Two capsules: parasitized and uninfected\n","\n","# Create the model\n","model = Model(inputs=input_layer, outputs=output_layer)\n","\n","# Compile the model with Adam optimizer and custom loss function\n","optimizer = Adam(learning_rate=0.007, beta_1=0.8)\n","loss_fn = custom_loss_function # Define the custom loss function as described in the paper\n","model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n","\n","# Print the model summary\n","model.summary()"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4","mount_file_id":"1Dnsl078JzGwH-snRqYpe4v_RDBNGZeJb","authorship_tag":"ABX9TyOlNkMGrHTegDqrvln1U6i0"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}