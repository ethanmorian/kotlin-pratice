{"cells":[{"cell_type":"markdown","metadata":{"id":"SoOMF4kHSwMS"},"source":["# Skin lesion classification of dermoscopic images using machine learning and convolutional neural network\n","\n","19 December 2022\n","\n","https://www.nature.com/articles/s41598-022-22644-9#Tab7\n","\n","https://aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=realm&dataSetSn=561"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3051,"status":"ok","timestamp":1693369781384,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"EzLpCjbDOZy_","outputId":"28f3cf38-4b6e-424a-9392-93a5710b1411"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import cv2\n","import glob\n","import json\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas as pd\n","import re\n","import tensorflow as tf\n","from sklearn.metrics import classification_report\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras import models\n","from tensorflow.keras.layers import BatchNormalization, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tqdm import tqdm"],"metadata":{"id":"491U9sI3TVVj","executionInfo":{"status":"ok","timestamp":1693369784860,"user_tz":-540,"elapsed":3480,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def get_image_and_json_paths(src_path):\n","    image_paths = sorted(glob.glob(os.path.join(src_path, '**', '*.jpg'), recursive=True))\n","    json_paths = sorted(glob.glob(os.path.join(src_path, '**', '*.json'), recursive=True))\n","\n","    return image_paths, json_paths\n","\n","def extract_data_from_json(json_paths):\n","    lesions, polygon_locations, box_locations = [], [], []\n","    for json_path in tqdm(json_paths, desc='Loading JSON', unit=' file'):\n","        try:\n","            with open(json_path, 'r', encoding='utf-8') as file:\n","                json_data = json.loads(re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', file.read()))\n","                metadata, labeling_info = json_data.get('metaData', None), json_data['labelingInfo']\n","\n","                lesions.append(metadata.get('lesions', None))\n","\n","                for entry in labeling_info:\n","                    if 'polygon' in entry:\n","                        polygon_locations.extend(entry['polygon'].get('location', None))\n","                    if 'box' in entry:\n","                        box_locations.extend(entry['box'].get('location', None))\n","\n","        except Exception as e:\n","            print(json_path)\n","            print(e)\n","\n","    return lesions, polygon_locations, box_locations\n","\n","def combine_images_and_masks(image_list, mask_list):\n","    combined_images = []\n","\n","    for i in tqdm(range(len(image_list)), desc='Combining Images and Masks'):\n","        image = image_list[i]\n","        mask = mask_list[i]\n","\n","        mask_channel = np.zeros_like(image[:, :, 0], dtype=np.uint8)\n","\n","        mask_channel[mask == 1] = 255\n","\n","        combined_image = np.dstack((image, mask_channel))\n","\n","        combined_images.append(combined_image)\n","\n","    return combined_images\n","\n","def one_hot_encode_labels(labels):\n","    label_binarizer = LabelBinarizer()\n","    encoded_labels = label_binarizer.fit_transform(labels)\n","    return np.array(encoded_labels), label_binarizer\n","\n","def create_polygon_binary_masks(image_list, polygon_locations_list):\n","    binary_masks = []\n","\n","    for image, polygon_locations in tqdm(zip(image_list, polygon_locations_list), desc='Generating Binary Masks', total=len(image_list)):\n","        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n","\n","        for poly_coords in polygon_locations:\n","            if poly_coords:\n","                poly_points = []\n","                i = 1\n","                while f'x{i}' in poly_coords and f'y{i}' in poly_coords:\n","                    x = poly_coords[f'x{i}']\n","                    y = poly_coords[f'y{i}']\n","                    poly_points.append([x, y])\n","                    i += 1\n","                if len(poly_points) > 0:\n","                    poly_points = np.array(poly_points, dtype=np.int32)\n","                    cv2.fillPoly(mask, [poly_points], 255)\n","            print(poly_coords)\n","\n","        binary_masks.append(mask)\n","\n","    return binary_masks\n","\n","def visualize_masks(image_data, mask_data, selected_indices, figsize=(10, 5)):\n","    for idx in selected_indices:\n","        image = image_data[idx]\n","        mask_map = mask_data[idx]\n","\n","        plt.figure(figsize=figsize)\n","        plt.subplot(1, mask_map.shape[2] + 1, 1)\n","        plt.imshow(image)\n","        plt.title('Original Image')\n","\n","        for ch in range(mask_map.shape[2]):\n","            plt.subplot(1, mask_map.shape[2] + 1, ch + 2)\n","            plt.imshow(mask_map[:, :, ch], cmap='gray')\n","            plt.title(f'Channel {ch}')\n","\n","        plt.tight_layout()\n","        plt.show()"],"metadata":{"id":"qEuSRFdOTKoU","executionInfo":{"status":"ok","timestamp":1693369784860,"user_tz":-540,"elapsed":5,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sALkX2o9Syb_"},"source":["# camera cat"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1693369784860,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"yH2t11ewBsJe"},"outputs":[],"source":["src_path = '/content/drive/Shareddrives/반려묘/일반카메라'"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":822,"status":"ok","timestamp":1693369785679,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"8tKyLk1IGIEw"},"outputs":[],"source":["image_paths, json_paths = get_image_and_json_paths(src_path)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_EHfOaAWGRRY","outputId":"999a625c-28cc-40d6-af55-c057332f83b9","executionInfo":{"status":"ok","timestamp":1693369797875,"user_tz":-540,"elapsed":12200,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Loading JSON: 100%|██████████| 15000/15000 [00:11<00:00, 1272.33 file/s]\n"]}],"source":["lesions, polygon_locations, box_locations = extract_data_from_json(json_paths)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ZQ90HinzOLeE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693369797876,"user_tz":-540,"elapsed":22,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"ae0c23ed-ca62-4688-e000-a536e95441a2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['A7', 'A7', 'A7', 'A7', 'A7']"]},"metadata":{},"execution_count":7}],"source":["lesions[:5]"]},{"cell_type":"code","source":["polygon_locations[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7S1RepxDkry4","executionInfo":{"status":"ok","timestamp":1693369797877,"user_tz":-540,"elapsed":19,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"071f80e9-8277-4f47-f2f3-afd984acf92f"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'x1': 370,\n"," 'y1': 291,\n"," 'x2': 311,\n"," 'y2': 322,\n"," 'x3': 310,\n"," 'y3': 338,\n"," 'x4': 415,\n"," 'y4': 336,\n"," 'x5': 515,\n"," 'y5': 281,\n"," 'x6': 607,\n"," 'y6': 246,\n"," 'x7': 647,\n"," 'y7': 222,\n"," 'x8': 628,\n"," 'y8': 221,\n"," 'x9': 548,\n"," 'y9': 221,\n"," 'x10': 370,\n"," 'y10': 291}"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["box_locations[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZUMZdUw1krvh","executionInfo":{"status":"ok","timestamp":1693369797878,"user_tz":-540,"elapsed":17,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"a7aafd67-8b8f-41b2-eac5-4684d30818f8"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'x': 310, 'y': 221, 'width': 337, 'height': 117},\n"," {'x': 1295, 'y': 605, 'width': 310, 'height': 201},\n"," {'x': 944, 'y': 384, 'width': 174, 'height': 72},\n"," {'x': 620, 'y': 412, 'width': 56, 'height': 112},\n"," {'x': 1261, 'y': 298, 'width': 75, 'height': 123}]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["df = pd.DataFrame({'image_path': image_paths, 'lesion': lesions})\n","\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"Kr-yYQQRlxFE","executionInfo":{"status":"ok","timestamp":1693369797878,"user_tz":-540,"elapsed":15,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"1578f010-b9fa-4eeb-84a0-5c12a0ad8887"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                              image_path lesion\n","0      /content/drive/Shareddrives/반려묘/일반카메라/무증상...     A7\n","1      /content/drive/Shareddrives/반려묘/일반카메라/무증상...     A7\n","2      /content/drive/Shareddrives/반려묘/일반카메라/무증상...     A7\n","3      /content/drive/Shareddrives/반려묘/일반카메라/무증상...     A7\n","4      /content/drive/Shareddrives/반려묘/일반카메라/무증상...     A7\n","...                                                  ...    ...\n","14995  /content/drive/Shareddrives/반려묘/일반카메라/유증상...     A6\n","14996  /content/drive/Shareddrives/반려묘/일반카메라/유증상...     A6\n","14997  /content/drive/Shareddrives/반려묘/일반카메라/유증상...     A6\n","14998  /content/drive/Shareddrives/반려묘/일반카메라/유증상...     A6\n","14999  /content/drive/Shareddrives/반려묘/일반카메라/유증상...     A6\n","\n","[15000 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-67e7e600-3bab-43dc-af67-530d3cfdca96\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_path</th>\n","      <th>lesion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/무증상...</td>\n","      <td>A7</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/무증상...</td>\n","      <td>A7</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/무증상...</td>\n","      <td>A7</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/무증상...</td>\n","      <td>A7</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/무증상...</td>\n","      <td>A7</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>14995</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/유증상...</td>\n","      <td>A6</td>\n","    </tr>\n","    <tr>\n","      <th>14996</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/유증상...</td>\n","      <td>A6</td>\n","    </tr>\n","    <tr>\n","      <th>14997</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/유증상...</td>\n","      <td>A6</td>\n","    </tr>\n","    <tr>\n","      <th>14998</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/유증상...</td>\n","      <td>A6</td>\n","    </tr>\n","    <tr>\n","      <th>14999</th>\n","      <td>/content/drive/Shareddrives/반려묘/일반카메라/유증상...</td>\n","      <td>A6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>15000 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67e7e600-3bab-43dc-af67-530d3cfdca96')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-67e7e600-3bab-43dc-af67-530d3cfdca96 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-67e7e600-3bab-43dc-af67-530d3cfdca96');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-05e6d6a5-5e67-4054-92d2-27108bf0befa\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-05e6d6a5-5e67-4054-92d2-27108bf0befa')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const charts = await google.colab.kernel.invokeFunction(\n","          'suggestCharts', [key], {});\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-05e6d6a5-5e67-4054-92d2-27108bf0befa button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["datagen = ImageDataGenerator(rescale=1.0/255)"],"metadata":{"id":"rCDslFuU79Ds","executionInfo":{"status":"ok","timestamp":1693369797879,"user_tz":-540,"elapsed":13,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"],"metadata":{"id":"O7m1-ZfmV-01","executionInfo":{"status":"ok","timestamp":1693369797879,"user_tz":-540,"elapsed":13,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["train_generator = datagen.flow_from_dataframe(\n","    dataframe=train_df,\n","    x_col='image_path',\n","    y_col='lesion',\n","    class_mode='categorical',\n","    target_size=(96, 96),\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p1IU9hs473GM","executionInfo":{"status":"ok","timestamp":1693369799954,"user_tz":-540,"elapsed":2088,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"1f67c64d-6d27-43b8-ba33-de3bd94e444f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 12000 validated image filenames belonging to 4 classes.\n"]}]},{"cell_type":"code","source":["val_generator = datagen.flow_from_dataframe(\n","    dataframe=val_df,\n","    x_col='image_path',\n","    y_col='lesion',\n","    class_mode='categorical',\n","    target_size=(96, 96),\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hS9LF1g3U3cu","executionInfo":{"status":"ok","timestamp":1693369799955,"user_tz":-540,"elapsed":11,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"c84ade63-f4b1-4a37-bc96-5042e6c92a51"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1500 validated image filenames belonging to 4 classes.\n"]}]},{"cell_type":"code","source":["test_generator = datagen.flow_from_dataframe(\n","    dataframe=test_df,\n","    x_col='image_path',\n","    y_col='lesion',\n","    class_mode='categorical',\n","    target_size=(96, 96),\n","    shuffle=False\n",")"],"metadata":{"id":"mXLi75CKV3Mk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693369799955,"user_tz":-540,"elapsed":9,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"07f17945-0c09-4b80-8940-5101302769b1"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1500 validated image filenames belonging to 4 classes.\n"]}]},{"cell_type":"code","source":["model = models.Sequential()\n","\n","model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(96,96,3)))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(3, 3)))\n","\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model.add(Dropout(0.25))\n","\n","model.add(Flatten())\n","\n","model.add(Dense(units=1024, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.5))\n","\n","model.add(Dense(units=4, activation='softmax'))\n","\n","opt = Adam(learning_rate=0.001)\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])"],"metadata":{"id":"JGUbxOe3TxF0","executionInfo":{"status":"ok","timestamp":1693369801778,"user_tz":-540,"elapsed":1828,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["history1 = model.fit(\n","    train_generator,\n","    epochs=150,\n","    batch_size=64,\n","    validation_data=val_generator\n",")"],"metadata":{"id":"2iSRx9X0aFUi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b95177db-5187-4a18-aac8-b46be8a5046b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/150\n"," 96/375 [======>.......................] - ETA: 21:35 - loss: 1.1007 - accuracy: 0.5723"]}]},{"cell_type":"code","source":["y_pred = model.predict(test_generator)\n","y_pred_classes = np.argmax(y_pred, axis=1)\n","y_true_classes = test_generator.classes\n","\n","print(classification_report(y_true_classes, y_pred_classes))"],"metadata":{"id":"_QCLXIZcTyct","executionInfo":{"status":"aborted","timestamp":1693374289582,"user_tz":-540,"elapsed":17,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TensorFlow Lite 모델로 변환\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","tflite_model = converter.convert()\n","\n","# 변환된 모델을 파일로 저장\n","with open('72.tflite', 'wb') as f:\n","    f.write(tflite_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7f5CnkuvE2g","executionInfo":{"status":"ok","timestamp":1693202560336,"user_tz":-540,"elapsed":7443,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"7183ee6a-4113-42c6-96b5-d1e77feedd93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"]}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from skimage.feature import greycomatrix\n","import h5py\n","\n","class ImageFeatureExtractor:\n","    def __init__(self, target_size=(96, 96)):\n","        self.target_size = target_size\n","\n","    def preprocess_image(self, image_path):\n","        image = cv2.imread(image_path)\n","        image = cv2.resize(image, self.target_size)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        return image\n","\n","    def extract_color_histogram(self, image):\n","        histogram = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n","        histogram = cv2.normalize(histogram, histogram).flatten()\n","        return histogram\n","\n","    def extract_hu_moments(self, image):\n","        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n","        moments = cv2.HuMoments(cv2.moments(gray_image)).flatten()\n","        return moments\n","\n","    def extract_haralick_texture(self, image):\n","        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n","        texture = greycomatrix(gray_image, distances=[1], angles=[0], symmetric=True, normed=True)\n","        haralick_features = np.mean(np.array(greycomatrix_features), axis=(0, 1, 2))\n","        return haralick_features\n","\n","    def extract_features(self, image_path):\n","        image = self.preprocess_image(image_path)\n","        color_histogram = self.extract_color_histogram(image)\n","        hu_moments = self.extract_hu_moments(image)\n","        haralick_texture = self.extract_haralick_texture(image)\n","        global_features = np.concatenate([color_histogram, hu_moments, haralick_texture])\n","        return global_features\n","\n","    def save_features_to_hdf5(self, features, labels, output_path):\n","        with h5py.File(output_path, 'w') as f:\n","            f.create_dataset('features', data=features)\n","            f.create_dataset('labels', data=labels)\n","\n","# Usage\n","json_paths = [...]  # List of JSON file paths\n","image_paths = [...]  # List of image file paths\n","labels = [...]  # List of corresponding labels\n","\n","feature_extractor = ImageFeatureExtractor(target_size=(96, 96))\n","\n","features = []\n","for image_path in image_paths:\n","    image_features = feature_extractor.extract_features(image_path)\n","    features.append(image_features)\n","\n","features = np.array(features)\n","labels = np.array(labels)\n","\n","output_path = 'features.h5'\n","feature_extractor.save_features_to_hdf5(features, labels, output_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335},"id":"2CiVGFj7bjdH","executionInfo":{"status":"error","timestamp":1693202567153,"user_tz":-540,"elapsed":1042,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"outputId":"f1c5ede8-3fca-4003-cbd3-a8d6fbc2af2b"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-0a3ba5c7fd9c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcombined_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lesions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 )\n\u001b[1;32m    721\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m                 mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    723\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;31m# by definition an array here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_ndarraylike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_on_sanitize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_dtype_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_prep_ndarraylike\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Must pass 2-d input. shape={values.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Must pass 2-d input. shape=(5500, 96, 96, 3)"]}]},{"cell_type":"code","source":["train_images, test_images, train_labels, test_labels = train_test_split(combined_df, encoded_labels, test_size=0.2, random_state=42)\n","test_images, val_images, test_labels, val_labels = train_test_split(test_images, test_labels, test_size=0.5, random_state=42)"],"metadata":{"id":"xZO2cyEhYavU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history2 = model.fit(train_images, train_labels, epochs=150, batch_size=32, validation_data=(val_images, val_labels))"],"metadata":{"id":"ig6GvlEyYaqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = model.predict(test_images)\n","y_pred_classes = np.argmax(y_pred, axis=1)\n","y_true_classes = np.argmax(test_labels, axis=1)\n","\n","print(classification_report(y_true_classes, y_pred_classes))"],"metadata":{"id":"tm9LepXEYalM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5BhTlrhC2uf8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# microscope cat"],"metadata":{"id":"5St-XgMdS3A_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZOVWv80Sea6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EG-vxlAj_HYD"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f0qJBmnqSbr7"},"outputs":[],"source":["from tensorflow.keras.layers import PReLU\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import MeanSquaredError\n","from tensorflow.keras.layers import Concatenate, MaxPooling2D, BatchNormalization\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.layers import concatenate\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.losses import mean_squared_error\n","\n","def inception_module(input_layer, filters):\n","    conv1x1 = Conv2D(filters[0], (1, 1), activation='relu')(input_layer)\n","    conv3x3_reduce = Conv2D(filters[1], (1, 1), activation='relu')(input_layer)\n","    conv3x3 = Conv2D(filters[2], (3, 3), padding='same', activation='relu')(conv3x3_reduce)\n","    conv5x5_reduce = Conv2D(filters[3], (1, 1), activation='relu')(input_layer)\n","    conv5x5 = Conv2D(filters[4], (5, 5), padding='same', activation='relu')(conv5x5_reduce)\n","    maxpool = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_layer)\n","    maxpool_conv = Conv2D(filters[5], (1, 1), activation='relu')(maxpool)\n","    inception_output = Concatenate(axis=-1)([conv1x1, conv3x3, conv5x5, maxpool_conv])\n","    return inception_output\n","\n","# Input\n","input_shape = (128, 128, 3)\n","input_layer = Input(shape=input_shape)\n","\n","# Inception block\n","inception_output = inception_module(input_layer, filters=[64, 128, 192, 32, 96, 64])\n","inception_output = inception_module(inception_output, filters=[64, 128, 192, 32, 96, 64])\n","# Add more inception modules if needed\n","\n","# Primary Capsule layer\n","primary_capsules = Conv2D(32, (1, 1), activation='relu')(inception_output)\n","\n","# Higher Capsule layers\n","# (Add imperative routing mechanism layers here)\n","\n","# PReLU activation for routing\n","higher_capsules_prelu = PReLU()(higher_capsules)\n","\n","# Flatten and Fully Connected layers\n","capsule_flatten = Flatten()(higher_capsules_prelu)  # Flatten higher capsules\n","output_layer = Dense(2, activation='softmax')(capsule_flatten)  # Two capsules: parasitized and uninfected\n","\n","# Create the model\n","model = Model(inputs=input_layer, outputs=output_layer)\n","\n","# Compile the model with Adam optimizer and custom loss function\n","optimizer = Adam(learning_rate=0.007, beta_1=0.8)\n","loss_fn = custom_loss_function # Define the custom loss function as described in the paper\n","model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n","\n","# Print the model summary\n","model.summary()"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"gpuType":"A100","mount_file_id":"1Dnsl078JzGwH-snRqYpe4v_RDBNGZeJb","authorship_tag":"ABX9TyNSMAD5ytGwO3KkDrbEasfP"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}