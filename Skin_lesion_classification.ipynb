{"cells":[{"cell_type":"markdown","metadata":{"id":"SoOMF4kHSwMS"},"source":["# Skin lesion classification of dermoscopic images using machine learning and convolutional neural network\n","\n","19 December 2022\n","\n","https://www.nature.com/articles/s41598-022-22644-9#Tab7\n","\n","https://aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=realm&dataSetSn=561"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38908,"status":"ok","timestamp":1692834754727,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"EzLpCjbDOZy_","outputId":"f8c2de2c-b477-4d3f-b4a6-d5009c72e578"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["def get_image_and_json_paths(src_path):\n","    image_paths = sorted(glob.glob(os.path.join(src_path, '**', '*.jpg'), recursive=True))\n","    json_paths = sorted(glob.glob(os.path.join(src_path, '**', '*.json'), recursive=True))\n","\n","    return image_paths, json_paths\n","\n","def get_image_data(image_paths):\n","    return [cv2.imread(image_path) for image_path in tqdm(image_paths, desc='Loading Images')]\n","\n","def extract_metadata_and_locations_from_json(json_paths):\n","    meta_data, polygon_data, box_data = [], [], []\n","    for json_path in tqdm(json_paths, desc='Loading JSON', unit=' file'):\n","        try:\n","            with open(json_path, \"r\", encoding=\"utf-8\") as file:\n","                json_data = json.loads(re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', file.read()))\n","                labeling_info, metadata = json_data['labelingInfo'], json_data.get('metaData', None)\n","\n","                filtered_metadata = {\n","                    'breed': metadata.get('breed', None),\n","                    'age': metadata.get('age', None),\n","                    'gender': metadata.get('gender', None),\n","                    'region': metadata.get('region', None),\n","                    'lesions': metadata.get('lesions', None)\n","                }\n","\n","                for entry in labeling_info:\n","                    if 'polygon' in entry:\n","                        polygon_data.append(entry['polygon'].get('location', None))\n","                    if 'box' in entry:\n","                        box_data.append(entry['box'].get('location', None))\n","\n","                meta_data.append(filtered_metadata)\n","        except Exception as e:\n","            print(f\"Error occurred while processing file: {json_path}\")\n","            print(f\"Error message: {e}\")\n","\n","    return meta_data, polygon_data, box_data"],"metadata":{"id":"qEuSRFdOTKoU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# camera dog"],"metadata":{"id":"AwmyldgvSrRS"}},{"cell_type":"markdown","metadata":{"id":"sALkX2o9Syb_"},"source":["# camera cat"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1692834754728,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"fRfHAxbnUa_b"},"outputs":[],"source":["import cv2\n","import glob\n","import json\n","import numpy as np\n","import os\n","from tqdm import tqdm\n","import re\n","import pandas as pd\n","from google.colab.patches import cv2_imshow"]},{"cell_type":"code","source":["\n","\n","def create_mask_maps(image_data, polygon_data, box_data):\n","    mask_maps = []\n","\n","    for image in tqdm(image_data, desc='Generating Mask Maps for Each Image'):\n","        mask_map = np.zeros(image.shape[:2] + (4,), dtype=np.uint8)\n","        fill_value = 1\n","\n","        for entry in polygon_data:\n","            if 'polygon' in entry:\n","                loc = entry['polygon']['location']\n","                if loc:\n","                    loc_array = [[coord['x'], coord['y']] for coord in loc]\n","                    polygon_points = np.array(loc_array, np.int32).reshape((-1, 1, 2))\n","                    cv2.fillPoly(mask_map[..., 0], [polygon_points], 255)\n","                    cv2.fillPoly(mask_map[..., 1], [polygon_points], fill_value)\n","\n","        for entry in box_data:\n","            if 'box' in entry:\n","                loc = entry['box']['location']\n","                if loc:\n","                    cv2.rectangle(mask_map[..., 2], tuple(loc[:2]), tuple(loc[2:]), 255, thickness=-1)\n","                    cv2.rectangle(mask_map[..., 3], tuple(loc[:2]), tuple(loc[2:]), fill_value, thickness=-1)\n","\n","        mask_maps.append(mask_map)\n","\n","    return mask_maps\n","\n","def combine_images_and_masks(image_list, mask_list):\n","    combined_images = []\n","\n","    for i in tqdm(range(len(image_list)), desc='Combining Images and Masks'):\n","        image = image_list[i]\n","        mask = mask_list[i]\n","\n","        combined_image = np.concatenate([image, mask[:, :, np.newaxis]], axis=-1)\n","        combined_images.append(combined_image)\n","\n","    return combined_images\n","\n","def resize_images(images, width, height):\n","    return [cv2.resize(image, (width, height), interpolation=cv2.INTER_AREA) for image in tqdm(images, desc='Resizing Images')]"],"metadata":{"id":"lYFHcr1U5sHN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1692834754729,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"yH2t11ewBsJe"},"outputs":[],"source":["src_path = \"/content/drive/Shareddrives/반려묘\""]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":57188,"status":"ok","timestamp":1692834811912,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"8tKyLk1IGIEw"},"outputs":[],"source":["image_paths, json_paths = get_image_and_json_paths(src_path)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6397895,"status":"ok","timestamp":1692841209802,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"XTJXZ1I1GR8N","outputId":"fc2e032f-859d-4847-c0f3-dfc8dc6d0d39"},"outputs":[{"output_type":"stream","name":"stderr","text":["Loading Images: 100%|██████████| 5500/5500 [1:46:37<00:00,  1.16s/it]\n"]}],"source":["image_data = get_image_data(image_paths)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_EHfOaAWGRRY","outputId":"654d00d1-dc83-4bc7-9fb2-2f83ed9a09b7"},"outputs":[{"output_type":"stream","name":"stderr","text":["Loading JSON:  51%|█████▏    | 2821/5500 [28:25<29:41,  1.50 file/s]"]}],"source":["meta_data, polygon_data, box_data = extract_metadata_and_locations_from_json(json_paths)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"78TBbsaKGM3x"},"outputs":[],"source":["masks_maps = create_mask_maps(image_data, polygon_data, box_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BLMFD5LYP8pk"},"outputs":[],"source":["polygon_binary_masks = [mask_map[..., 0] for mask_map in masks_maps]"]},{"cell_type":"code","source":["polygon_binary_masks = combine_images_and_masks(image_data, polygon_binary_masks)"],"metadata":{"id":"RaURtFgMhFGQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def check_image_mask_properties(image_list, mask_list):\n","    for i in range(len(image_list)):\n","        image = image_list[i]\n","        mask = mask_list[i]\n","\n","        print(f\"Image {i + 1}:\")\n","        print(f\"  Image Shape: {image.shape}\")\n","        print(f\"  Mask Shape: {mask.shape}\")\n","\n","        if image.shape[:2] != mask.shape[:2]:\n","            print(\"  Image and mask shapes do not match!\")\n","\n","        if image.shape[2] != 3:\n","            print(\"  Image does not have 3 color channels!\")\n","\n","        if mask.shape[2] != 1:\n","            print(\"  Mask does not have 1 channel!\")\n","\n","        print()\n","\n","# 이미지와 마스크 데이터의 속성 확인\n","check_image_mask_properties(image_data, polygon_binary_masks)"],"metadata":{"id":"OD0cbG1pzCYt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["polygon_segmentation_maps = [mask_map[..., 1] for mask_map in masks_maps]"],"metadata":{"id":"_I7AVH-MisQL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["polygon_segmentation_maps = combine_images_and_masks(image_data, polygon_segmentation_maps)"],"metadata":{"id":"kV6xJJQEicpZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["box_binary_masks = [mask_map[..., 2] for mask_map in masks_maps]"],"metadata":{"id":"dy0te6AziVVu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["box_binary_masks = combine_images_and_masks(image_data, box_binary_masks)"],"metadata":{"id":"osoJaVCxiVQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["box_segmentation_maps = [mask_map[..., 3] for mask_map in masks_maps]"],"metadata":{"id":"9AiDr1RPiVMQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["box_segmentation_maps = combine_images_and_masks(image_data, box_segmentation_maps)"],"metadata":{"id":"N84mIlfpimqW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0MWhZQ90GMyH"},"outputs":[],"source":["original_size_mask_maps = resize_images(masks_maps, 96, 96)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YiVFErvYGMs5"},"outputs":[],"source":["resized_mask_maps = create_mask_maps(resize_images(image_data, 96, 96), polygon_data, box_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MlalA8N-GMSd"},"outputs":[],"source":["metadata_df = pd.DataFrame(meta_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQ90HinzOLeE"},"outputs":[],"source":["metadata_df"]},{"cell_type":"markdown","metadata":{"id":"auDavqpSS5rc"},"source":["# microscope dog"]},{"cell_type":"markdown","source":["# microscope cat"],"metadata":{"id":"5St-XgMdS3A_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9KN24A9E1JFc"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import models\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.layers import BatchNormalization, Dropout, Flatten, Dense\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.metrics import classification_report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4QcwK9UBb2u"},"outputs":[],"source":["train_images, test_images, train_labels, test_labels = train_test_split(resize_images(image_data, 96, 96), metadata_df['lesions'], test_size=0.3, random_state=42)\n","test_images, val_images, test_labels, val_labels = train_test_split(test_images, test_labels, test_size=0.5, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZOVWv80Sea6"},"outputs":[],"source":["# Sequential 모델 생성\n","model = models.Sequential()\n","\n","# Conv2D 레이어와 MaxPooling2D 레이어 추가\n","model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(96,96,3)))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(3, 3)))\n","\n","# Dropout 레이어 추가\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model.add(Dropout(0.25))\n","\n","# Flatten 레이어\n","model.add(Flatten())\n","\n","# Dense 레이어와 Dropout 레이어 추가\n","model.add(Dense(units=1024, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.5))\n","\n","# 최종 출력 레이어\n","model.add(Dense(units=7, activation='softmax'))\n","\n","# 모델 컴파일\n","opt = Adam(lr=0.001, decay=0.00001)\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","# 모델 구조 요약\n","model.summary()\n","\n","# 모델 학습\n","epochs = 150\n","batch_size = 32\n","\n","history = model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(val_images, val_labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EG-vxlAj_HYD"},"outputs":[],"source":["# 모델 평가\n","test_loss, test_acc = model.evaluate(test_images, test_labels)\n","print(\"Test Loss:\", test_loss)\n","print(\"Test Accuracy:\", test_acc)\n","\n","# 분류 모델을 평가할 수 있는 다양한 지표 계산\n","y_pred = model.predict(test_images)\n","y_pred_classes = np.argmax(y_pred, axis=1)\n","y_true_classes = np.argmax(test_labels, axis=1)\n","\n","# classification_report를 사용하여 분류 모델의 성능 평가\n","print(classification_report(y_true_classes, y_pred_classes))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f0qJBmnqSbr7"},"outputs":[],"source":["# TensorFlow Lite 모델로 변환\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","tflite_model = converter.convert()\n","\n","# 변환된 모델을 파일로 저장\n","with open('your_model.tflite', 'wb') as f:\n","    f.write(tflite_model)"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"toc_visible":true,"gpuType":"A100","authorship_tag":"ABX9TyNNoA6tYFxU86u6BNGIY9tb"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}