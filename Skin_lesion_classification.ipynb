{"cells":[{"cell_type":"markdown","metadata":{"id":"SoOMF4kHSwMS"},"source":["# camera cat\n","\n","Skin lesion classification of dermoscopic images using machine learning and convolutional neural network\n","\n","19 December 2022\n","\n","https://www.nature.com/articles/s41598-022-22644-9#Tab7\n","\n","https://aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=realm&dataSetSn=561"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13718,"status":"ok","timestamp":1697414824150,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"EzLpCjbDOZy_","outputId":"b408927b-e46a-4def-fe2a-9754ed821718"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"491U9sI3TVVj"},"outputs":[],"source":["import os\n","import glob\n","import cv2\n","import numpy as np\n","from tqdm import tqdm\n","\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n","from sklearn.preprocessing import LabelEncoder\n","\n","from skimage.feature import graycomatrix\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Concatenate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qEuSRFdOTKoU"},"outputs":[],"source":["class ImageLoader:\n","    def __init__(self, src_path, num_files_per_folder):\n","        self.src_path = src_path\n","        self.num_files_per_folder = num_files_per_folder\n","\n","    def get_path_and_label(self):\n","        image_paths = []\n","        dermatological_lesions = []\n","\n","        for root, dirs, files in tqdm(os.walk(self.src_path), desc='Walking directories', unit=' dir'):\n","\n","            for dir in dirs:\n","                dir_path = os.path.join(root, dir)\n","                image_files = sorted(glob.glob(os.path.join(dir_path, '*.jpg')))\n","\n","                for image_file in image_files[:self.num_files_per_folder]:\n","                    image_paths.append(image_file)\n","                    filename = os.path.basename(image_file)\n","                    parts = filename.split('_')\n","                    second_part = parts[2]\n","                    dermatological_lesions.append(second_part)\n","\n","        return image_paths, dermatological_lesions\n","\n","class ImagePreprocessor:\n","    def __init__(self, image_paths, target_size):\n","        self.image_paths = image_paths\n","        self.target_size = target_size\n","\n","    def preprocess_images(self):\n","        images = []\n","        gray_images = []\n","\n","        for image_path in tqdm(self.image_paths, desc='Preprocessing Images'):\n","            image = cv2.imread(image_path)\n","\n","            if image is None:\n","                print(f\"Error loading image: {image_path}\")\n","                continue\n","\n","            image = cv2.resize(image, self.target_size)\n","\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n","\n","            images.append(image)\n","            gray_images.append(gray_image)\n","\n","        return np.array(images), np.array(gray_images)\n","\n","class FeatureExtractor:\n","    def __init__(self, images, gray_images):\n","        self.images = images\n","        self.gray_images = gray_images\n","\n","    def extract_color_histograms(self):\n","        color_histograms = np.zeros((self.images.shape[0], 512))\n","\n","        for i, image in tqdm(enumerate(self.images), desc='Extracting Color Histograms'):\n","            histogram = cv2.calcHist([image], [0, 1, 2], None,[8 ,8 ,8 ], [0 ,256 ,0 ,256 ,0 ,256])\n","            histogram = cv2.normalize(histogram,histogram).flatten()\n","            color_histograms[i] = histogram\n","\n","        return color_histograms\n","\n","    def extract_hu_moments(self):\n","        moments_list = [cv2.HuMoments(cv2.moments(gray_image)).flatten() for gray_image in tqdm(self.gray_images)]\n","        hu_moments = np.array(moments_list)\n","\n","        return hu_moments\n","\n","    def extract_haralick_textures(self):\n","        textures_list = [np.mean(graycomatrix(gray_image, distances=[1], angles=[0], symmetric=True, normed=True), axis=(0, 1)).flatten() for gray_image in tqdm(self.gray_images, desc='Extracting Haralick Textures')]\n","        haralick_textures = np.array(textures_list)\n","\n","        return haralick_textures\n","\n","class DatasetCreator:\n","    def __init__(self, loader: ImageLoader,\n","                 preprocessor: ImagePreprocessor,\n","                 feature_extractor: FeatureExtractor):\n","\n","        self.loader = loader\n","        self.preprocessor = preprocessor\n","        self.feature_extractor = feature_extractor\n","\n","    def create_dataset(self):\n","        self.get_path_and_label()\n","        self.preprocess_images()\n","        self.extract_color_histograms()\n","        self.extract_hu_moments()\n","        self.extract_haralick_textures()\n","\n","        self.feature_vectors = np.concatenate([self.color_histograms,\n","                                        self.hu_moments,\n","                                        self.haralick_textures], axis=1)\n","\n","        encoder = LabelEncoder()\n","        encoded_labels = encoder.fit_transform(self.lesions)\n","        num_classes = len(encoder.classes_)\n","        one_hot_labels = tf.one_hot(encoded_labels, depth=num_classes)\n","\n","        self.image = tf.convert_to_tensor(self.images, dtype=tf.float32)\n","        self.image = self.image / tf.constant(255.0, dtype=tf.float32)\n","\n","        self.dataset = tf.data.Dataset.from_tensor_slices(({\"image_input\": self.image,\n","                                                    \"global_feature_input\": self.feature_vectors},\n","                                                    one_hot_labels))\n","\n","class Pipeline:\n","    def __init__(self, loader: ImageLoader,\n","                 preprocessor: ImagePreprocessor,\n","                 feature_extractor: FeatureExtractor):\n","\n","       self.loader = loader\n","       self.preprocessor = preprocessor\n","       self.feature_extractor = feature_extractor\n","\n","    def execute(self):\n","       for image in self.loader.load_images():\n","           preprocessed_image = self.preprocessor.preprocess_image(image)\n","           features = self.feature_extractor.extract_features(preprocessed_image)\n","           yield features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"executionInfo":{"elapsed":37,"status":"error","timestamp":1695615272586,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"yH2t11ewBsJe","outputId":"68128345-16db-4683-c04d-10c77040a76a"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-7d169a9f22d5>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mimage_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlesions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_path_and_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_files_per_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'get_path_and_label' is not defined"]}],"source":["src_path = '/content/drive/Shareddrives/반려묘/일반카메라/Training'\n","num_files_per_folder = 2000\n","target_size = (256, 256)\n","buffer_size = 1000\n","batch_size = 64\n","\n","image_paths, lesions = get_path_and_label(src_path, num_files_per_folder)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":556044,"status":"ok","timestamp":1695191379613,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"AP9hy-9_CyBe","outputId":"cbb8782f-5e8d-48da-ab33-619ba596335b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Preprocessing Images: 100%|██████████| 20490/20490 [09:01<00:00, 37.85it/s]\n","Extracting Color Histograms: 20490it [00:03, 5848.99it/s]\n","Extracting Hu Moments: 100%|██████████| 20490/20490 [00:01<00:00, 15993.48it/s]\n","Extracting Haralick Textures: 100%|██████████| 20490/20490 [00:08<00:00, 2517.30it/s]\n"]}],"source":["image_feature_vectors = extract_features(image_paths)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1695191379616,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"JvahrmpVUEjx","outputId":"ab39a019-a2b7-450c-e8ab-6169bffdca0d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of image paths:  20490\n","Number of image feature vectors:  20490\n","Number of labels:  20490\n"]}],"source":["print('Number of image paths: ', len(image_paths))\n","print('Number of image feature vectors: ', len(image_feature_vectors))\n","print('Number of labels: ', len(lesions))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJ9Te2aqmIuB"},"outputs":[],"source":["dataset = create_dataset(image_paths, image_feature_vectors, lesions)\n","dataset = dataset.shuffle(buffer_size).batch(batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"23mCdag7SVAd"},"outputs":[],"source":["dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n","\n","train_ratio = 0.8\n","val_ratio = 0.1\n","test_ratio = 0.1\n","\n","train_size = int(train_ratio * dataset_size)\n","val_size = int(val_ratio * dataset_size)\n","test_size = int(test_ratio * dataset_size)\n","\n","train_dataset = dataset.take(train_size)\n","val_dataset_temp = dataset.skip(train_size)\n","\n","val_dataset = val_dataset_temp.take(val_size)\n","test_dataset = val_dataset_temp.skip(val_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":476,"status":"ok","timestamp":1695191604420,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"1wDcy8gyUZuu","outputId":"55967847-49b7-44aa-b50a-e8d12960b4e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Expected train_dataset size:  256\n","Expected val_dataset size:  32\n","Expected test_dataset size:  32\n","Actual train_dataset size:  256\n","Actual val_dataset size:  32\n","Actual test_dataset size:  33\n"]}],"source":["print('Expected train_dataset size: ', train_size)\n","print('Expected val_dataset size: ', val_size)\n","print('Expected test_dataset size: ', test_size)\n","\n","print('Actual train_dataset size: ', tf.data.experimental.cardinality(train_dataset).numpy())\n","print('Actual val_dataset size: ', tf.data.experimental.cardinality(val_dataset).numpy())\n","print('Actual test_dataset size: ', tf.data.experimental.cardinality(test_dataset).numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JGUbxOe3TxF0"},"outputs":[],"source":["image_input = Input(shape=(256, 256, 3), name='image_input')\n","x = Conv2D(32, (3, 3), padding='same', activation='relu')(image_input)\n","x = BatchNormalization()(x)\n","x = MaxPooling2D(pool_size=(3 , 3))(x)\n","\n","x = Dropout(0.25)(x)\n","\n","x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n","x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n","x = BatchNormalization()(x)\n","x = MaxPooling2D(pool_size=(2 , 2))(x)\n","\n","x = Dropout(0.25)(x)\n","\n","x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n","x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n","x = BatchNormalization()(x)\n","x = MaxPooling2D(pool_size=(2 , 2))(x)\n","\n","x = Dropout(0.25)(x)\n","\n","global_feature_input = Input(shape=(520,), name='global_feature_input')\n","\n","combined_features= Concatenate()([Flatten()(x), global_feature_input])\n","\n","combined_features= Dense(1024, activation='relu')(combined_features)\n","combined_features= BatchNormalization ()(combined_features)\n","combined_features= Dropout(0.5)(combined_features)\n","\n","outputs= Dense(4, activation='softmax')(combined_features)\n","\n","model= Model(inputs=[image_input, global_feature_input], outputs=outputs)\n","\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646},"executionInfo":{"elapsed":61362,"status":"error","timestamp":1695189089081,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"2iSRx9X0aFUi","outputId":"a95cf439-b970-4aa7-9156-18b9a06110b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/150\n","219/256 [========================>.....] - ETA: 9s - loss: 1.5696 - accuracy: 0.5117"]},{"ename":"InvalidArgumentError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-a274e7d75fef>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  jpeg::Uncompress failed. Invalid JPEG data or crop window.\n\t [[{{node DecodeJpeg}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_4]]\n  (1) INVALID_ARGUMENT:  jpeg::Uncompress failed. Invalid JPEG data or crop window.\n\t [[{{node DecodeJpeg}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_6919]"]}],"source":["history = model.fit(\n","    train_dataset,\n","    validation_data=val_dataset,\n","    epochs=150,\n","    batch_size=64\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_QCLXIZcTyct"},"outputs":[],"source":["y_true = np.concatenate([y.numpy() for x, y in test_dataset], axis=0)\n","y_pred = model.predict(test_dataset)\n","y_pred_classes = np.argmax(y_pred, axis=1)\n","y_true_classes = np.argmax(y_true, axis=1)\n","\n","print(classification_report(y_true_classes, y_pred_classes))\n","\n","print(confusion_matrix(y_true_classes, y_pred_classes))\n","\n","roc_auc = roc_auc_score(y_true, y_pred , multi_class='ovr')\n","print('ROC-AUC score:', roc_auc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s7f5CnkuvE2g"},"outputs":[],"source":["# TensorFlow Lite 모델로 변환\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","tflite_model = converter.convert()\n","\n","# 변환된 모델을 파일로 저장\n","with open('3000_256.tflite', 'wb') as f:\n","    f.write(tflite_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EeLaIH1wZTsL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qqg4JHS0Y_yH"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"5St-XgMdS3A_"},"source":["# microscope cat\n","\n","Intelligent diagnostic model for malaria parasite detection and classification using imperative inception-based capsule neural networks\n","\n","17 August 2023\n","\n","https://www.nature.com/articles/s41598-023-40317-z#Sec3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VlK5SbTNjCBq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f0qJBmnqSbr7"},"outputs":[],"source":["from tensorflow.keras.layers import PReLU\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import MeanSquaredError\n","from tensorflow.keras.layers import Concatenate, MaxPooling2D, BatchNormalization\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.layers import concatenate\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.losses import mean_squared_error\n","\n","def inception_module(input_layer, filters):\n","    conv1x1 = Conv2D(filters[0], (1, 1), activation='relu')(input_layer)\n","    conv3x3_reduce = Conv2D(filters[1], (1, 1), activation='relu')(input_layer)\n","    conv3x3 = Conv2D(filters[2], (3, 3), padding='same', activation='relu')(conv3x3_reduce)\n","    conv5x5_reduce = Conv2D(filters[3], (1, 1), activation='relu')(input_layer)\n","    conv5x5 = Conv2D(filters[4], (5, 5), padding='same', activation='relu')(conv5x5_reduce)\n","    maxpool = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_layer)\n","    maxpool_conv = Conv2D(filters[5], (1, 1), activation='relu')(maxpool)\n","    inception_output = Concatenate(axis=-1)([conv1x1, conv3x3, conv5x5, maxpool_conv])\n","    return inception_output\n","\n","# Input\n","input_shape = (128, 128, 3)\n","input_layer = Input(shape=input_shape)\n","\n","# Inception block\n","inception_output = inception_module(input_layer, filters=[64, 128, 192, 32, 96, 64])\n","inception_output = inception_module(inception_output, filters=[64, 128, 192, 32, 96, 64])\n","# Add more inception modules if needed\n","\n","# Primary Capsule layer\n","primary_capsules = Conv2D(32, (1, 1), activation='relu')(inception_output)\n","\n","# Higher Capsule layers\n","# (Add imperative routing mechanism layers here)\n","\n","# PReLU activation for routing\n","higher_capsules_prelu = PReLU()(higher_capsules)\n","\n","# Flatten and Fully Connected layers\n","capsule_flatten = Flatten()(higher_capsules_prelu)  # Flatten higher capsules\n","output_layer = Dense(2, activation='softmax')(capsule_flatten)  # Two capsules: parasitized and uninfected\n","\n","# Create the model\n","model = Model(inputs=input_layer, outputs=output_layer)\n","\n","# Compile the model with Adam optimizer and custom loss function\n","optimizer = Adam(learning_rate=0.007, beta_1=0.8)\n","loss_fn = custom_loss_function # Define the custom loss function as described in the paper\n","model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n","\n","# Print the model summary\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"G2utFcGBsi0a"},"source":["# symptom classification"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5237,"status":"ok","timestamp":1698021263047,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"1Trs04oJkau6"},"outputs":[],"source":["import os\n","import multiprocessing\n","\n","import glob\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import models\n","from torchvision import transforms\n","from torchvision.io import read_image"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1698021263048,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"OUC-ubX4kalc"},"outputs":[],"source":["class ImageLoader:\n","    def __init__(self, src_path, num_files_per_folder):\n","        self.src_path = src_path\n","        self.num_files_per_folder = num_files_per_folder\n","\n","    def get_image_paths_and_labels(self):\n","        image_paths = []\n","        symptom_labels = []\n","\n","        for root, dirs, files in os.walk(self.src_path):\n","            for dir in dirs:\n","                dir_path = os.path.join(root, dir)\n","                image_files = sorted(glob.glob(os.path.join(dir_path, '*.jpg')))\n","\n","                for image_file in image_files[:self.num_files_per_folder]:\n","                    image_paths.append(image_file)\n","                    filename = os.path.basename(image_file)\n","                    symptom_labels.append(0 if \"A7\" in filename else 1)\n","\n","        return image_paths, symptom_labels\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, image_paths, labels, transform):\n","        self.image_paths = image_paths\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self,idx):\n","        try:\n","            img = read_image(self.image_paths[idx])\n","            img = img.float() / 255\n","            img = self.transform(img)\n","            label = torch.tensor(int(self.labels[idx]),dtype=torch.long)\n","            return img,label\n","\n","        except Exception as e:\n","            print(f\"Error loading image: {self.image_paths[idx]}, {str(e)}\")\n","\n","class DataPipeline:\n","    def __init__(self, src_path, num_files_per_folder, target_size, batch_size):\n","        loader = ImageLoader(src_path=src_path, num_files_per_folder=num_files_per_folder)\n","        self.image_paths, self.labels = loader.get_image_paths_and_labels()\n","\n","        data_transforms=transforms.Compose([transforms.Resize(target_size, antialias=True)])\n","        dataset = CustomDataset(self.image_paths, self.labels, transform=data_transforms)\n","\n","        print(f\"Size of the Dataset: {len(dataset)}\")\n","\n","        total_size = len(dataset)\n","        train_size = int(0.7 * total_size)\n","        valid_size = int(0.15 * total_size)\n","        test_size = total_size - train_size - valid_size\n","\n","        self.train_dataset, self.valid_dataset, self.test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n","\n","        self.train_dataloader = DataLoader(self.train_dataset,\n","                                      batch_size=batch_size,\n","                                      shuffle=True,\n","                                      num_workers=multiprocessing.cpu_count())\n","\n","        self.valid_dataloader = DataLoader(self.valid_dataset,\n","                                      batch_size=batch_size,\n","                                      num_workers=multiprocessing.cpu_count())\n","\n","        self.test_dataloader = DataLoader(self.test_dataset,\n","                                     batch_size=batch_size,\n","                                     num_workers=multiprocessing.cpu_count())\n","\n","    def get_dataloader(self):\n","        for dataloader in [self.train_dataloader, self.valid_dataloader, self.test_dataloader]:\n","            if isinstance(dataloader, torch.utils.data.DataLoader):\n","                print(f\"{dataloader} creation successful\")\n","            else:\n","                print(f\"{dataloader} creation failed\")\n","\n","            try:\n","                images, labels = next(iter(dataloader))\n","                print(\"Batch data retrieval successful\")\n","                print(f\"Images shape: {images.shape}\")\n","                print(f\"Labels shape: {labels.shape}\")\n","            except Exception as e:\n","                print(\"Batch data retrieval failed:\", str(e))\n","\n","        return self.train_dataloader, self.valid_dataloader, self.test_dataloader"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":612648,"status":"ok","timestamp":1698021875686,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"nOHS0etFk1cS","outputId":"6b8e97c2-08d4-4e35-ba5d-cf9896c1c534"},"outputs":[{"output_type":"stream","name":"stdout","text":["Size of the Dataset: 5500\n","<torch.utils.data.dataloader.DataLoader object at 0x78b28758bb50> creation successful\n","Batch data retrieval successful\n","Images shape: torch.Size([64, 3, 256, 256])\n","Labels shape: torch.Size([64])\n","<torch.utils.data.dataloader.DataLoader object at 0x78b2878754e0> creation successful\n","Batch data retrieval successful\n","Images shape: torch.Size([64, 3, 256, 256])\n","Labels shape: torch.Size([64])\n","<torch.utils.data.dataloader.DataLoader object at 0x78b28758bb20> creation successful\n","Batch data retrieval successful\n","Images shape: torch.Size([64, 3, 256, 256])\n","Labels shape: torch.Size([64])\n"]}],"source":["pipeline = DataPipeline(src_path=\"/content/drive/Shareddrives/반려묘/일반카메라\",\n","                        num_files_per_folder=500,\n","                        target_size=(256, 256),\n","                        batch_size=64)\n","\n","train_dataloader, valid_dataloader, test_dataloader = pipeline.get_dataloader()"]},{"cell_type":"code","source":["class ImageClassifier:\n","    def __init__(self, num_epochs, early_stopping_patience):\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        self.model = models.resnet50(pretrained=True)\n","        num_ftrs = self.model.fc.in_features\n","        self.model.fc = nn.Linear(num_ftrs, 2)\n","        self.model.to(self.device)\n","\n","        if torch.cuda.device_count() > 1:\n","            print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n","            self.model = nn.DataParallel(self.model)\n","\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.optimizer_ft = torch.optim.AdamW(self.model.parameters(), lr=0.001)\n","\n","        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer_ft, step_size=7, gamma=0.1)\n","\n","        self.num_epochs = num_epochs\n","        self.patience = early_stopping_patience\n","\n","    def train_and_validate(self, dataloader, is_train):\n","        total_loss = 0.0\n","        total_corrects = 0\n","\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(self.device), labels.to(self.device)\n","\n","            if is_train:\n","                self.model.train()\n","                torch.set_grad_enabled(True)\n","            else:\n","                self.model.eval()\n","                torch.set_grad_enabled(False)\n","\n","            outputs = self.model(inputs)\n","            _, preds = torch.max(outputs, dim=1)\n","\n","            loss = self.criterion(outputs, labels)\n","\n","            if is_train:\n","                loss.backward()\n","                self.optimizer_ft.step()\n","\n","            total_loss += loss.item() * inputs.size(0)\n","            total_corrects += torch.sum(preds == labels.data).item()\n","\n","        epoch_loss = total_loss / len(dataloader.dataset)\n","        epoch_accuarcy = total_corrects / len(dataloader.dataset)\n","\n","        return epoch_loss, epoch_accuarcy\n","\n","    def train(self, train_dataloader, valid_dataloader):\n","\n","        best_model_wts = copy.deepcopy(self.model.state_dict())\n","        best_accuracy_val_score = -1\n","\n","        patience_counter = 0\n","\n","        for epoch in range(self.num_epochs):\n","\n","            train_loss ,train_accuracy=self.train_and_validate(train_dataloader,is_train=True)\n","            val_loss,val_accuracy=self.train_and_validate(valid_dataloader,is_train=False)\n","\n","            print(f\"Epoch [{epoch+1}/{self.num_epochs}] Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} Valid Loss: {val_loss:.4f}, Valid Acc: {val_accuracy:.4f}\")\n","\n","            if val_accuracy > best_accuracy_val_score :\n","                best_accuracy_val_score=val_accuracy\n","                patience_counter=0\n","                best_model_wts=self.model.state_dict()\n","\n","            else :\n","                patience_counter+=1\n","\n","            if patience_counter>self.patience :\n","                print('Early Stopping!')\n","                break\n","\n","        self.model.load_state_dict(best_model_wts)\n","\n","    def test(self, test_dataloader):\n","        y_true_classes=[]\n","        y_pred_classes=[]\n","        y_pred_proba=[]\n","\n","        with torch.no_grad():\n","            for inputs, labels in test_dataloader:\n","                inputs, labels = inputs.to(self.device), labels.to(self.device)\n","\n","                outputs = self.model(inputs)\n","                probas = torch.nn.functional.softmax(outputs, dim=1)\n","\n","                _, preds = torch.max(probas, dim=1)\n","\n","                y_true_classes.extend(labels.detach().cpu().numpy())\n","                y_pred_classes.extend(preds.cpu().numpy())\n","\n","                y_pred_proba.extend(probas[:, 1].cpu().numpy())\n","\n","        print(classification_report(y_true_classes,y_pred_classes))\n","        print(confusion_matrix(y_true_classes,y_pred_classes))\n","\n","        roc_auc = roc_auc_score(y_true_classes, y_pred_proba)\n","        print('ROC-AUC score:', roc_auc)"],"metadata":{"id":"1uwsZn_yF4jh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4421200,"status":"ok","timestamp":1698026296866,"user":{"displayName":"이보원","userId":"15276212846060170538"},"user_tz":-540},"id":"yRgkYjVlw9th","outputId":"77ce14e8-deeb-46d6-ddb1-d760b01e50f2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 192MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/150] Train Loss: 0.5566, Train Acc: 0.7475 Valid Loss: 0.5528, Valid Acc: 0.7248\n","Epoch [2/150] Train Loss: 0.4963, Train Acc: 0.7721 Valid Loss: 0.5272, Valid Acc: 0.7552\n","Epoch [3/150] Train Loss: 0.4668, Train Acc: 0.7945 Valid Loss: 0.5077, Valid Acc: 0.7770\n","Epoch [4/150] Train Loss: 0.4506, Train Acc: 0.8015 Valid Loss: 0.5484, Valid Acc: 0.7612\n","Epoch [5/150] Train Loss: 0.4264, Train Acc: 0.8103 Valid Loss: 0.5484, Valid Acc: 0.7867\n","Epoch [6/150] Train Loss: 0.4331, Train Acc: 0.8137 Valid Loss: 0.4980, Valid Acc: 0.7745\n","Epoch [7/150] Train Loss: 0.4043, Train Acc: 0.8293 Valid Loss: 0.5049, Valid Acc: 0.7891\n","Epoch [8/150] Train Loss: 0.3658, Train Acc: 0.8420 Valid Loss: 0.5172, Valid Acc: 0.7709\n","Epoch [9/150] Train Loss: 0.3398, Train Acc: 0.8563 Valid Loss: 0.5090, Valid Acc: 0.7988\n","Epoch [10/150] Train Loss: 0.3468, Train Acc: 0.8491 Valid Loss: 0.5297, Valid Acc: 0.7733\n","Epoch [11/150] Train Loss: 0.3372, Train Acc: 0.8561 Valid Loss: 0.5677, Valid Acc: 0.7842\n","Epoch [12/150] Train Loss: 0.2964, Train Acc: 0.8794 Valid Loss: 0.5615, Valid Acc: 0.7758\n","Epoch [13/150] Train Loss: 0.3119, Train Acc: 0.8613 Valid Loss: 0.5833, Valid Acc: 0.7952\n","Epoch [14/150] Train Loss: 0.2643, Train Acc: 0.8909 Valid Loss: 0.6964, Valid Acc: 0.7733\n","Epoch [15/150] Train Loss: 0.2421, Train Acc: 0.9026 Valid Loss: 0.6105, Valid Acc: 0.7588\n","Epoch [16/150] Train Loss: 0.1909, Train Acc: 0.9228 Valid Loss: 0.7409, Valid Acc: 0.7648\n","Epoch [17/150] Train Loss: 0.1708, Train Acc: 0.9288 Valid Loss: 0.7682, Valid Acc: 0.7588\n","Epoch [18/150] Train Loss: 0.1562, Train Acc: 0.9418 Valid Loss: 0.7379, Valid Acc: 0.7842\n","Epoch [19/150] Train Loss: 0.1065, Train Acc: 0.9623 Valid Loss: 0.9092, Valid Acc: 0.7661\n","Epoch [20/150] Train Loss: 0.1059, Train Acc: 0.9582 Valid Loss: 0.9800, Valid Acc: 0.7891\n","Epoch [21/150] Train Loss: 0.1632, Train Acc: 0.9392 Valid Loss: 0.9095, Valid Acc: 0.7515\n","Epoch [22/150] Train Loss: 0.0773, Train Acc: 0.9709 Valid Loss: 0.9825, Valid Acc: 0.7636\n","Epoch [23/150] Train Loss: 0.2407, Train Acc: 0.9065 Valid Loss: 0.7083, Valid Acc: 0.7624\n","Epoch [24/150] Train Loss: 0.0960, Train Acc: 0.9644 Valid Loss: 1.0046, Valid Acc: 0.7624\n","Epoch [25/150] Train Loss: 0.0269, Train Acc: 0.9922 Valid Loss: 1.2305, Valid Acc: 0.7794\n","Epoch [26/150] Train Loss: 0.1118, Train Acc: 0.9582 Valid Loss: 0.9548, Valid Acc: 0.7503\n","Epoch [27/150] Train Loss: 0.0907, Train Acc: 0.9675 Valid Loss: 1.0673, Valid Acc: 0.7770\n","Epoch [28/150] Train Loss: 0.0621, Train Acc: 0.9805 Valid Loss: 1.1462, Valid Acc: 0.7624\n","Epoch [29/150] Train Loss: 0.0322, Train Acc: 0.9888 Valid Loss: 1.2290, Valid Acc: 0.7733\n","Epoch [30/150] Train Loss: 0.0198, Train Acc: 0.9935 Valid Loss: 1.3854, Valid Acc: 0.7697\n","Epoch [31/150] Train Loss: 0.2104, Train Acc: 0.9231 Valid Loss: 0.8221, Valid Acc: 0.7661\n","Epoch [32/150] Train Loss: 0.0595, Train Acc: 0.9782 Valid Loss: 1.1388, Valid Acc: 0.7673\n","Epoch [33/150] Train Loss: 0.0310, Train Acc: 0.9901 Valid Loss: 1.3589, Valid Acc: 0.7721\n","Epoch [34/150] Train Loss: 0.0175, Train Acc: 0.9940 Valid Loss: 1.2804, Valid Acc: 0.7842\n","Epoch [35/150] Train Loss: 0.0122, Train Acc: 0.9966 Valid Loss: 1.4783, Valid Acc: 0.7576\n","Epoch [36/150] Train Loss: 0.1217, Train Acc: 0.9595 Valid Loss: 1.0901, Valid Acc: 0.7588\n","Epoch [37/150] Train Loss: 0.0861, Train Acc: 0.9680 Valid Loss: 1.2065, Valid Acc: 0.7697\n","Epoch [38/150] Train Loss: 0.0224, Train Acc: 0.9932 Valid Loss: 1.3335, Valid Acc: 0.7721\n","Epoch [39/150] Train Loss: 0.0303, Train Acc: 0.9909 Valid Loss: 1.3313, Valid Acc: 0.7624\n","Epoch [40/150] Train Loss: 0.0140, Train Acc: 0.9964 Valid Loss: 1.5443, Valid Acc: 0.7648\n","Epoch [41/150] Train Loss: 0.0202, Train Acc: 0.9940 Valid Loss: 1.4369, Valid Acc: 0.7855\n","Epoch [42/150] Train Loss: 0.0098, Train Acc: 0.9961 Valid Loss: 1.7259, Valid Acc: 0.7782\n","Epoch [43/150] Train Loss: 0.0712, Train Acc: 0.9735 Valid Loss: 1.2778, Valid Acc: 0.7576\n","Epoch [44/150] Train Loss: 0.1184, Train Acc: 0.9577 Valid Loss: 1.1033, Valid Acc: 0.7467\n","Epoch [45/150] Train Loss: 0.0483, Train Acc: 0.9834 Valid Loss: 1.2434, Valid Acc: 0.7636\n","Epoch [46/150] Train Loss: 0.0148, Train Acc: 0.9958 Valid Loss: 1.3594, Valid Acc: 0.7842\n","Epoch [47/150] Train Loss: 0.0042, Train Acc: 0.9992 Valid Loss: 1.4564, Valid Acc: 0.7770\n","Epoch [48/150] Train Loss: 0.0011, Train Acc: 1.0000 Valid Loss: 1.5274, Valid Acc: 0.7770\n","Epoch [49/150] Train Loss: 0.0037, Train Acc: 0.9987 Valid Loss: 1.6638, Valid Acc: 0.7770\n","Epoch [50/150] Train Loss: 0.0049, Train Acc: 0.9987 Valid Loss: 1.6179, Valid Acc: 0.7685\n","Epoch [51/150] Train Loss: 0.0055, Train Acc: 0.9987 Valid Loss: 1.6941, Valid Acc: 0.7745\n","Epoch [52/150] Train Loss: 0.0323, Train Acc: 0.9867 Valid Loss: 1.5759, Valid Acc: 0.7442\n","Epoch [53/150] Train Loss: 0.0676, Train Acc: 0.9753 Valid Loss: 1.1665, Valid Acc: 0.7721\n","Epoch [54/150] Train Loss: 0.0193, Train Acc: 0.9938 Valid Loss: 1.4579, Valid Acc: 0.7661\n","Epoch [55/150] Train Loss: 0.0181, Train Acc: 0.9938 Valid Loss: 1.5353, Valid Acc: 0.7685\n","Epoch [56/150] Train Loss: 0.0077, Train Acc: 0.9979 Valid Loss: 1.7025, Valid Acc: 0.7782\n","Epoch [57/150] Train Loss: 0.0109, Train Acc: 0.9964 Valid Loss: 1.6387, Valid Acc: 0.7685\n","Epoch [58/150] Train Loss: 0.1841, Train Acc: 0.9395 Valid Loss: 0.9587, Valid Acc: 0.7867\n","Epoch [59/150] Train Loss: 0.0521, Train Acc: 0.9810 Valid Loss: 1.0692, Valid Acc: 0.7818\n","Epoch [60/150] Train Loss: 0.0481, Train Acc: 0.9826 Valid Loss: 1.3233, Valid Acc: 0.7515\n","Epoch [61/150] Train Loss: 0.0317, Train Acc: 0.9932 Valid Loss: 1.6108, Valid Acc: 0.7624\n","Epoch [62/150] Train Loss: 0.3259, Train Acc: 0.8883 Valid Loss: 0.7187, Valid Acc: 0.7467\n","Epoch [63/150] Train Loss: 0.0721, Train Acc: 0.9771 Valid Loss: 1.1190, Valid Acc: 0.7697\n","Epoch [64/150] Train Loss: 0.0312, Train Acc: 0.9891 Valid Loss: 1.2403, Valid Acc: 0.7709\n","Epoch [65/150] Train Loss: 0.0652, Train Acc: 0.9756 Valid Loss: 1.2936, Valid Acc: 0.7600\n","Epoch [66/150] Train Loss: 0.0352, Train Acc: 0.9857 Valid Loss: 1.2570, Valid Acc: 0.7697\n","Epoch [67/150] Train Loss: 0.0060, Train Acc: 0.9990 Valid Loss: 1.6406, Valid Acc: 0.7600\n","Epoch [68/150] Train Loss: 0.0039, Train Acc: 0.9984 Valid Loss: 1.9092, Valid Acc: 0.7588\n","Epoch [69/150] Train Loss: 0.1681, Train Acc: 0.9415 Valid Loss: 0.9365, Valid Acc: 0.7539\n","Epoch [70/150] Train Loss: 0.0298, Train Acc: 0.9922 Valid Loss: 1.1738, Valid Acc: 0.7661\n","Epoch [71/150] Train Loss: 0.0080, Train Acc: 0.9987 Valid Loss: 1.3495, Valid Acc: 0.7648\n","Epoch [72/150] Train Loss: 0.0029, Train Acc: 0.9997 Valid Loss: 1.4224, Valid Acc: 0.7685\n","Epoch [73/150] Train Loss: 0.0014, Train Acc: 1.0000 Valid Loss: 1.4690, Valid Acc: 0.7697\n","Epoch [74/150] Train Loss: 0.0015, Train Acc: 0.9997 Valid Loss: 1.5047, Valid Acc: 0.7636\n","Epoch [75/150] Train Loss: 0.0713, Train Acc: 0.9774 Valid Loss: 1.2617, Valid Acc: 0.7479\n","Epoch [76/150] Train Loss: 0.0357, Train Acc: 0.9870 Valid Loss: 1.4542, Valid Acc: 0.7721\n","Epoch [77/150] Train Loss: 0.0123, Train Acc: 0.9961 Valid Loss: 1.5678, Valid Acc: 0.7758\n","Epoch [78/150] Train Loss: 0.0030, Train Acc: 0.9992 Valid Loss: 1.5757, Valid Acc: 0.7636\n","Epoch [79/150] Train Loss: 0.0024, Train Acc: 0.9995 Valid Loss: 1.6074, Valid Acc: 0.7648\n","Epoch [80/150] Train Loss: 0.0008, Train Acc: 1.0000 Valid Loss: 1.6652, Valid Acc: 0.7697\n","Epoch [81/150] Train Loss: 0.0003, Train Acc: 1.0000 Valid Loss: 1.7073, Valid Acc: 0.7709\n","Epoch [82/150] Train Loss: 0.0004, Train Acc: 1.0000 Valid Loss: 1.7287, Valid Acc: 0.7648\n","Epoch [83/150] Train Loss: 0.0004, Train Acc: 1.0000 Valid Loss: 1.7349, Valid Acc: 0.7685\n","Epoch [84/150] Train Loss: 0.0007, Train Acc: 1.0000 Valid Loss: 1.7379, Valid Acc: 0.7697\n","Epoch [85/150] Train Loss: 0.0003, Train Acc: 1.0000 Valid Loss: 1.7694, Valid Acc: 0.7697\n","Epoch [86/150] Train Loss: 0.0011, Train Acc: 0.9997 Valid Loss: 1.8140, Valid Acc: 0.7600\n","Epoch [87/150] Train Loss: 0.0019, Train Acc: 0.9987 Valid Loss: 1.9448, Valid Acc: 0.7491\n","Epoch [88/150] Train Loss: 0.1872, Train Acc: 0.9335 Valid Loss: 0.9583, Valid Acc: 0.7515\n","Epoch [89/150] Train Loss: 0.0515, Train Acc: 0.9818 Valid Loss: 1.0823, Valid Acc: 0.7600\n","Epoch [90/150] Train Loss: 0.0112, Train Acc: 0.9977 Valid Loss: 1.3180, Valid Acc: 0.7636\n","Epoch [91/150] Train Loss: 0.0051, Train Acc: 0.9984 Valid Loss: 1.3839, Valid Acc: 0.7503\n","Epoch [92/150] Train Loss: 0.0357, Train Acc: 0.9893 Valid Loss: 1.4740, Valid Acc: 0.7503\n","Epoch [93/150] Train Loss: 0.0161, Train Acc: 0.9943 Valid Loss: 1.6234, Valid Acc: 0.7564\n","Epoch [94/150] Train Loss: 0.0057, Train Acc: 0.9977 Valid Loss: 1.7493, Valid Acc: 0.7442\n","Epoch [95/150] Train Loss: 0.0229, Train Acc: 0.9922 Valid Loss: 1.7062, Valid Acc: 0.7552\n","Epoch [96/150] Train Loss: 0.1477, Train Acc: 0.9501 Valid Loss: 1.0468, Valid Acc: 0.7297\n","Epoch [97/150] Train Loss: 0.0263, Train Acc: 0.9917 Valid Loss: 1.4461, Valid Acc: 0.7576\n","Epoch [98/150] Train Loss: 0.1023, Train Acc: 0.9678 Valid Loss: 1.1591, Valid Acc: 0.7600\n","Epoch [99/150] Train Loss: 0.0303, Train Acc: 0.9914 Valid Loss: 1.2781, Valid Acc: 0.7600\n","Epoch [100/150] Train Loss: 0.0921, Train Acc: 0.9688 Valid Loss: 1.2635, Valid Acc: 0.7527\n","Epoch [101/150] Train Loss: 0.0188, Train Acc: 0.9948 Valid Loss: 1.3825, Valid Acc: 0.7661\n","Epoch [102/150] Train Loss: 0.0033, Train Acc: 0.9995 Valid Loss: 1.4546, Valid Acc: 0.7612\n","Epoch [103/150] Train Loss: 0.0022, Train Acc: 0.9997 Valid Loss: 1.5470, Valid Acc: 0.7600\n","Epoch [104/150] Train Loss: 0.0009, Train Acc: 1.0000 Valid Loss: 1.6281, Valid Acc: 0.7624\n","Epoch [105/150] Train Loss: 0.0007, Train Acc: 1.0000 Valid Loss: 1.6672, Valid Acc: 0.7624\n","Epoch [106/150] Train Loss: 0.0038, Train Acc: 0.9992 Valid Loss: 1.6938, Valid Acc: 0.7612\n","Epoch [107/150] Train Loss: 0.0018, Train Acc: 0.9997 Valid Loss: 1.7501, Valid Acc: 0.7612\n","Epoch [108/150] Train Loss: 0.0017, Train Acc: 0.9997 Valid Loss: 1.7790, Valid Acc: 0.7527\n","Epoch [109/150] Train Loss: 0.1149, Train Acc: 0.9595 Valid Loss: 1.1938, Valid Acc: 0.7527\n","Epoch [110/150] Train Loss: 0.0303, Train Acc: 0.9917 Valid Loss: 1.3014, Valid Acc: 0.7588\n","Epoch [111/150] Train Loss: 0.0314, Train Acc: 0.9904 Valid Loss: 1.5360, Valid Acc: 0.7358\n","Epoch [112/150] Train Loss: 0.0050, Train Acc: 0.9992 Valid Loss: 1.5979, Valid Acc: 0.7515\n","Epoch [113/150] Train Loss: 0.0013, Train Acc: 1.0000 Valid Loss: 1.6795, Valid Acc: 0.7624\n","Epoch [114/150] Train Loss: 0.0008, Train Acc: 1.0000 Valid Loss: 1.7742, Valid Acc: 0.7588\n","Epoch [115/150] Train Loss: 0.0004, Train Acc: 1.0000 Valid Loss: 1.8194, Valid Acc: 0.7600\n","Epoch [116/150] Train Loss: 0.0011, Train Acc: 0.9997 Valid Loss: 1.9391, Valid Acc: 0.7491\n","Epoch [117/150] Train Loss: 0.0947, Train Acc: 0.9686 Valid Loss: 1.2910, Valid Acc: 0.7394\n","Epoch [118/150] Train Loss: 0.0278, Train Acc: 0.9909 Valid Loss: 1.3826, Valid Acc: 0.7479\n","Epoch [119/150] Train Loss: 0.0061, Train Acc: 0.9990 Valid Loss: 1.5269, Valid Acc: 0.7636\n","Epoch [120/150] Train Loss: 0.0355, Train Acc: 0.9860 Valid Loss: 1.6763, Valid Acc: 0.7248\n","Epoch [121/150] Train Loss: 0.0167, Train Acc: 0.9945 Valid Loss: 1.7207, Valid Acc: 0.7661\n","Epoch [122/150] Train Loss: 0.0348, Train Acc: 0.9904 Valid Loss: 1.6744, Valid Acc: 0.7588\n","Epoch [123/150] Train Loss: 0.1129, Train Acc: 0.9623 Valid Loss: 1.2910, Valid Acc: 0.7455\n","Epoch [124/150] Train Loss: 0.0221, Train Acc: 0.9940 Valid Loss: 1.4460, Valid Acc: 0.7467\n","Epoch [125/150] Train Loss: 0.0727, Train Acc: 0.9738 Valid Loss: 1.3029, Valid Acc: 0.7430\n","Epoch [126/150] Train Loss: 0.0102, Train Acc: 0.9979 Valid Loss: 1.4246, Valid Acc: 0.7479\n","Epoch [127/150] Train Loss: 0.0821, Train Acc: 0.9717 Valid Loss: 1.3814, Valid Acc: 0.7273\n","Epoch [128/150] Train Loss: 0.0475, Train Acc: 0.9831 Valid Loss: 1.3866, Valid Acc: 0.7527\n","Epoch [129/150] Train Loss: 0.0058, Train Acc: 0.9992 Valid Loss: 1.5015, Valid Acc: 0.7515\n","Epoch [130/150] Train Loss: 0.0013, Train Acc: 1.0000 Valid Loss: 1.5673, Valid Acc: 0.7552\n","Epoch [131/150] Train Loss: 0.0009, Train Acc: 1.0000 Valid Loss: 1.6200, Valid Acc: 0.7588\n","Epoch [132/150] Train Loss: 0.0014, Train Acc: 0.9997 Valid Loss: 1.6660, Valid Acc: 0.7539\n","Epoch [133/150] Train Loss: 0.0799, Train Acc: 0.9735 Valid Loss: 1.3190, Valid Acc: 0.7527\n","Epoch [134/150] Train Loss: 0.0157, Train Acc: 0.9961 Valid Loss: 1.4561, Valid Acc: 0.7612\n","Epoch [135/150] Train Loss: 0.0074, Train Acc: 0.9984 Valid Loss: 1.5486, Valid Acc: 0.7527\n","Epoch [136/150] Train Loss: 0.0040, Train Acc: 0.9992 Valid Loss: 1.6150, Valid Acc: 0.7564\n","Epoch [137/150] Train Loss: 0.0140, Train Acc: 0.9953 Valid Loss: 1.6433, Valid Acc: 0.7588\n","Epoch [138/150] Train Loss: 0.0026, Train Acc: 0.9995 Valid Loss: 1.6989, Valid Acc: 0.7564\n","Epoch [139/150] Train Loss: 0.0012, Train Acc: 1.0000 Valid Loss: 1.8170, Valid Acc: 0.7491\n","Epoch [140/150] Train Loss: 0.0006, Train Acc: 1.0000 Valid Loss: 1.8446, Valid Acc: 0.7467\n","Epoch [141/150] Train Loss: 0.0006, Train Acc: 0.9997 Valid Loss: 1.8837, Valid Acc: 0.7539\n","Epoch [142/150] Train Loss: 0.0456, Train Acc: 0.9855 Valid Loss: 1.7094, Valid Acc: 0.7624\n","Epoch [143/150] Train Loss: 0.0209, Train Acc: 0.9927 Valid Loss: 1.7638, Valid Acc: 0.7564\n","Epoch [144/150] Train Loss: 0.0101, Train Acc: 0.9969 Valid Loss: 1.8876, Valid Acc: 0.7455\n","Epoch [145/150] Train Loss: 0.0053, Train Acc: 0.9990 Valid Loss: 2.0915, Valid Acc: 0.7527\n","Epoch [146/150] Train Loss: 0.0042, Train Acc: 0.9987 Valid Loss: 2.1104, Valid Acc: 0.7382\n","Epoch [147/150] Train Loss: 0.0034, Train Acc: 0.9990 Valid Loss: 2.2048, Valid Acc: 0.7455\n","Epoch [148/150] Train Loss: 0.0028, Train Acc: 0.9987 Valid Loss: 2.0813, Valid Acc: 0.7588\n","Epoch [149/150] Train Loss: 0.0023, Train Acc: 0.9995 Valid Loss: 2.1633, Valid Acc: 0.7467\n","Epoch [150/150] Train Loss: 0.0106, Train Acc: 0.9961 Valid Loss: 2.1877, Valid Acc: 0.7394\n"]}],"source":["net = ImageClassifier(num_epochs=150, early_stopping_patience=5)\n","net.train(train_dataloader, valid_dataloader)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"EKSBh8F_IX04","executionInfo":{"status":"ok","timestamp":1698026341197,"user_tz":-540,"elapsed":44337,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"290d485a-813e-4f80-f7ba-d383e5f7fe37"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.80      0.76      0.78       483\n","           1       0.69      0.74      0.71       343\n","\n","    accuracy                           0.75       826\n","   macro avg       0.75      0.75      0.75       826\n","weighted avg       0.76      0.75      0.75       826\n","\n","[[369 114]\n"," [ 90 253]]\n","ROC-AUC score: 0.8095660624498247\n"]}],"source":["net.test(test_dataloader)"]},{"cell_type":"code","source":["torch.save(net.model, \"/content/drive/MyDrive/Classifier.pth\")"],"metadata":{"id":"zN1b_7elM9Fp","executionInfo":{"status":"ok","timestamp":1698026358319,"user_tz":-540,"elapsed":497,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class ModelPredictor:\n","    def __init__(self, model_path, target_size):\n","        self.model = torch.load(model_path)\n","        self.model.eval()\n","        self.target_size = target_size\n","\n","    def preprocess(self, image_path):\n","        img = read_image(image_path)\n","        img = img.float() / 255\n","        transform = transforms.Compose([transforms.Resize(self.target_size, antialias=True)])\n","        img = transform(img)\n","        img_tensor = img.unsqueeze(0).to(next(self.model.parameters()).device)\n","\n","        return img_tensor\n","\n","    def predict(self, image_path):\n","        with torch.no_grad():\n","            input_tensor = self.preprocess(image_path)\n","\n","            output_tensor = self.model(input_tensor)\n","\n","            probabilities = torch.nn.functional.softmax(output_tensor[0], dim=0)\n","\n","            predicted_class_index = torch.argmax(probabilities).item()\n","\n","            return predicted_class_index, probabilities[predicted_class_index].item()"],"metadata":{"id":"MXSNfrEEEWxW","executionInfo":{"status":"ok","timestamp":1698026381385,"user_tz":-540,"elapsed":3,"user":{"displayName":"이보원","userId":"15276212846060170538"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["predictor=ModelPredictor(model_path=\"/content/drive/MyDrive/Classifier.pth\", target_size=(256,256))\n","class_idx, probability=predictor.predict(\"/content/drive/MyDrive/IMG_C_A6_807768.jpg\")\n","print(f\"Predicted class: {class_idx}, Probability: {probability}\")"],"metadata":{"id":"BJYNpgOLM5qK","executionInfo":{"status":"ok","timestamp":1698026382958,"user_tz":-540,"elapsed":1077,"user":{"displayName":"이보원","userId":"15276212846060170538"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a2ac7d6c-8bde-4f89-8f52-c0d93686e7ab"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted class: 0, Probability: 0.9960675239562988\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"A100","mount_file_id":"1Dnsl078JzGwH-snRqYpe4v_RDBNGZeJb","authorship_tag":"ABX9TyNQBUSvNRA0Udn9zw7GrDKf"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}